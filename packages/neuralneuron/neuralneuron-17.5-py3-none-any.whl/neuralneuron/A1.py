def a1():
	print('''

# -*- coding: utf-8 -*-
"""Practical1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1fJKrEFUXLtFeUE0YBvyR_cy51InVkTOn
"""

import numpy as np

n = int(input("Enter number of inputs: "))

inputs = []
print("Enter inputs for neural network")
for i in range(n):
    x = float(input())
    inputs.append(x)
inputs_array = np.array(inputs)
print(inputs_array)

weights = []
print("Enter weights for neural network")
for i in range(n):
    x = float(input())
    weights.append(x)
weights_array = np.array(weights)
print(weights_array)

bias = float(input("Enter bias: "))

def linear(x):
    return x

def sigmoid(x):
    return ( 1 / (1 + np.exp(-x)) )

def relu(x):
    return np.maximum(0, x)

def tanh(x):
    return ( (2 / (1 + np.exp(-2*x))) -1 )

def nn(inputs, weights, bias, activation_function):
    weighted_sum = sum(x * w for x, w in zip(inputs, weights))
    yin = weighted_sum + bias
    output = activation_function(yin)
    return output

linear_activation = nn(inputs, weights, bias, linear)
print(f"Linear Output for inputs {inputs}: {linear_activation}")

sigmoid_activation = nn(inputs, weights, bias, sigmoid)
print(f"Sigmoid Output for inputs {inputs}: {sigmoid_activation}")

relu_activation = nn(inputs, weights, bias, relu)
print(f"ReLu Output for inputs {inputs}: {relu_activation}")

tanh_activation = nn(inputs, weights, bias, tanh)
print(f"tanh Output for inputs {inputs}: {tanh_activation}")

import numpy as np
import matplotlib.pyplot as plt

def linear(x):
    return x

def sigmoid(x):
    return ( 1 / (1 + np.exp(-x)) )

def relu(x):
    return np.maximum(0, x)

def tanh(x):
    return ( (2 / (1 + np.exp(-2*x))) -1 )

def softmax(x):
    exp_values = np.exp(x - np.max(x))
    return exp_values / np.sum(exp_values)

x = np.linspace(-6, 6, 100)

plt.figure(figsize= (12, 8))

plt.subplot(2, 3, 1)
plt.plot(x, linear(x), label='Linear', color='purple')
plt.title('Linear Activation Function')
plt.legend()

plt.subplot(2, 3, 2)
plt.plot(x, sigmoid(x), label='Sigmoid', color='blue')
plt.title('Sigmoid Activation Function')
plt.legend()

plt.subplot(2, 3, 3)
plt.plot(x, relu(x), label='ReLU', color='orange')
plt.title('ReLU Activation Function')
plt.legend()

plt.subplot(2, 3, 4)
plt.plot(x, tanh(x), label='Tanh', color='green')
plt.title('Tanh Activation Function')
plt.legend()

plt.subplot(2, 3, 5)
plt.plot(x, softmax(x), label='Softmax', color='red')
plt.title('Softmax Activation Function')
plt.legend()

plt.tight_layout()
plt.show()

import numpy as np
import math
from matplotlib import pyplot as plt

x=np.linspace(-5,5,100);

plt.plot(x,1/(1+np.exp(-x)),label="sigmoid")
plt.plot(x,np.maximum(0,x),label="ReLU")

tanhy=[]
for i in x:
    tanhy.append((math.exp(i)-math.exp(-i))/(math.exp(i)+math.exp(-i)))
plt.plot(x,tanhy,label="tanh");

plt.plot(x,x,label="identity")
plt.plot(x,np.exp(x)/np.sum(np.exp(x)),label="Softmax")




plt.legend()
plt.plot()
plt.show()


''')
a1()