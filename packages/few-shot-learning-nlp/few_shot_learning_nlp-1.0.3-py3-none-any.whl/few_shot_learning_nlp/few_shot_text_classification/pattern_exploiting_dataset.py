import torch
from torch.utils.data import Dataset
from typing import List, Dict
from transformers import AutoTokenizer
from copy import deepcopy
from tqdm import tqdm

class PETDatasetForClassification(Dataset):
    def __init__(
        self, 
        processed_text : List[str], 
        labels : List[int],
        tokenizer : AutoTokenizer,
        device : str = "cuda"
    ) -> None:
        """
        Dataset class for Pattern Exploiting Training (PET) in text classification tasks.

        Args:
            processed_text (List[str]): The list of processed input texts.
            labels (List[int]): The list of corresponding class labels.
            tokenizer (AutoTokenizer): The tokenizer used to tokenize the input texts.
            device (str, optional): The device on which to perform calculations ('cuda' or 'cpu'). Defaults to "cuda".

        Attributes:
            tokens (Dict[str, torch.Tensor]): Tokenized input texts.
            encoded_labels (torch.Tensor): Encoded labels for PET training.
            inputs (Dict[str, torch.Tensor]): Dictionary containing tokenized inputs and encoded labels.
                Keys: 'input_ids', 'attention_mask', 'labels'
            device (str): The device on which calculations are performed.

        Methods:
            __getitem__(index): Retrieves an item from the dataset at the specified index.
            __len__(): Returns the total number of items in the dataset.

        Notes:
            - This dataset class is designed for use with PET (Pattern Exploiting Training) in text classification tasks.
            - Each input text is tokenized using the provided tokenizer and padded/truncated to the maximum length.
            - The class labels are encoded and replaced with mask tokens for PET training.
            - The dataset is prepared for training on the specified device.
        """
        
        super().__init__()

        self.tokens = tokenizer(
            processed_text,
            return_tensors="pt",
            padding="max_length",
            truncation=True
        )
        self.encoded_labels = deepcopy(self.tokens['input_ids'])
        
        self.encoded_labels[self.encoded_labels != tokenizer.mask_token_id] = -100

        for idx, sentence in tqdm(enumerate(self.encoded_labels)):
            sentence[sentence == tokenizer.mask_token_id] = tokenizer.vocab[labels[idx].lower()]

        self.inputs : Dict[str, torch.Tensor] = self.tokens
        self.inputs['labels'] = self.encoded_labels

        for k,v in self.inputs.items():
            self.inputs[k] = v.to(device)

    def __getitem__(self, index):
        d : Dict = dict()
        for key in self.inputs.keys():
            d[key] = self.inputs[key][index]

        return d
    
    def __len__(self):
        return self.tokens['input_ids'].shape[0]