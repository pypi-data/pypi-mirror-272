from tqdm import tqdm
from torch.utils.data import Dataset
from typing import List, Dict
import torch
from copy import copy

class SplitWordsDataset(Dataset):
    def __init__(
        self,
        data,
        tokenizer,
        pattern_fn : callable,
        separators = [".", ":", "?"],
        label_names = None
    ) -> None:
        """
        A custom dataset class designed for processing textual data into a format suitable
        for few-shot learning by segmenting text based on specified separators and generating
        patterns using a given function.

        This class transforms a dataset containing words and associated named entity recognition (NER)
        tags into a new structure where each word or phrase is associated with a label and a pattern
        generated by a custom function. It is primarily used for preparing data for tasks like
        token classification where context and separation of sentences can be crucial.

        Parameters:
        -----------
        data : Dataset
            The input dataset containing at least the following columns:
            - 'words': a list of words (tokens) from the dataset.
            - 'ner_tags': a list of entity tags corresponding to the words in the 'words' list.
        tokenizer : PreTrainedTokenizer
            A tokenizer that is used for processing text data, typically from the `transformers` library.
        pattern_fn : callable
            A function that takes a phrase, a word, and a tokenizer, and returns a custom pattern.
            This function is used to transform each phrase into a structured format.
        separators : list of str, optional
            A list of punctuation marks or other characters used to split the 'words' into phrases.
            Defaults to [".", ":", "?"].
        label_names : list of str, optional
            A list of labels corresponding to the indices in 'ner_tags'. If not provided, it will
            be automatically extracted from the 'ner_tags' feature metadata in the data.

        Attributes:
        -----------
        processed_data : list
            The processed dataset containing patterns and labels for training or inference.

        Methods:
        --------
        process(data):
            Processes the raw dataset to generate a list of patterns and corresponding labels based
            on the initialized parameters.

        Example:
        --------
        >>> dataset = load_dataset("some_dataset")
        >>> tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")
        >>> pattern_function = lambda phrase, word, tokenizer: tokenizer.encode(phrase)
        >>> split_dataset = SplitWordsDataset(data=dataset, tokenizer=tokenizer, pattern_fn=pattern_function)
        >>> print(split_dataset[0])
        """
        super().__init__()

        self.tokenizer= tokenizer
        self.separators = separators

        self.pattern_fn = pattern_fn

        if label_names == None:
            self.label_names : List[str] = data\
                .features['ner_tags']\
                .feature\
                .names

        else:
            self.label_names = label_names
        self.label_keymap = {k:v for k,v in enumerate(self.label_names)} 

        self.raw_data = data

        self.process(data)

    def process(self, data):
        """
        Process the input data to extract and transform text into structured patterns and labels.

        Parameters:
        data : Dataset
            The dataset to process, as described in the class documentation.
        """
        self.processed_data = []

        for idx in tqdm(range(len(data))):
            example = data[idx]
            words = example['words']

            full_text = ' '.join(words)
            new_full_text = copy(full_text)

            for sep in self.separators:
                new_full_text = new_full_text.replace(sep, ".")

            split = SplitWordsDataset.split_string(
                new_full_text, 
                split_char=".", 
                min_words=3    
            )
            
            patterns = self.create_pattern(
                split,
                example['ner_tags'],
                words,
            )

            self.processed_data.append(patterns)

    @staticmethod
    def split_string(
        input_string: str, 
        split_char: str, 
        min_words: int = 3,
    ):
        """
        Splits a string into phrases based on the provided character, ensuring that each
        resulting phrase contains at least a minimum number of words.

        Parameters:
        -----------
        input_string : str
            The string to be split.
        split_char : str
            The character on which to split the string.
        min_words : int
            The minimum number of words required in each phrase.

        Returns:
        --------
        list of str
            A list of phrases split from the input string.
        """
        words = input_string.split(split_char)
        result = []
        current_split = []

        for word in words:
            # Check if adding the current word would exceed the minimum word count
            if len(current_split) + 1 <= min_words:
                current_split.append(word)
            else:
                # If adding the word exceeds the minimum count, start a new split
                result.append(split_char.join(current_split))
                current_split = [word]

        # Add the remaining words to the result
        if current_split:
            result.append(split_char.join(current_split))

        return result
    
    def create_pattern(
        self,
        split: List,
        targets : List,
        words,
    ):
        """
        Generate patterns and corresponding labels for each phrase in the split list.

        Parameters:
        -----------
        split : list of str
            The list of phrases to process.
        targets : list of int
            The list of target labels corresponding to the original words.
        words : list of str
            The original list of words.

        Returns:
        --------
        list of dict
            Each dictionary in the list contains a 'pattern' and a 'label' for a phrase.
        """
        pattern_list = []
        idx = 0
        for phrase in split:
            for word in phrase.split(" "):

                if len(word) < 2:
                    continue
                
                while word.split('.')[0] not in words[idx] :
                    idx += 1
                    
                label = targets[idx]
                real_name_label = self.label_keymap[label]
        
                if real_name_label == "O":
                    # continue
                    real_name_label= "none" 
                else :
                    real_name_label = real_name_label[2:].lower()

                pattern = self.pattern_fn(phrase, word, self.tokenizer)
                pattern_list.append({"pattern": pattern,
                                    "label": real_name_label})
        
        return pattern_list
    
    def get_processed_data(
        self,
        n_shots: int = -1
    ):
        """
        Retrieves the processed data from the dataset.

        This method returns the processed text patterns and corresponding labels
        for training or inference. The `process` method must be called before
        invoking this method to ensure that the dataset has been processed.

        Parameters:
        -----------
        n_shots : int, optional
            The number of examples to retrieve from the processed data. If set to -1 (default),
            all examples will be returned.

        Returns:
        --------
        tuple
            A tuple containing two lists:
            - The processed text patterns.
            - The corresponding labels for each pattern.

        Note:
        -----
        The `process` method must be called before invoking this method to ensure
        that the dataset has been processed and the processed data is available.

        Example:
        --------
        >>> split_dataset = SplitWordsDataset(data=dataset, tokenizer=tokenizer, pattern_fn=pattern_function)
        >>> processed_text, labels = split_dataset.get_processed_data(n_shots=100)
        >>> print(len(processed_text), len(labels))
        """
        processed_text = []

        labels = []

        for document_text in self.processed_data[0:n_shots]:
            for phrase in document_text:
                processed_text.append(phrase['pattern'])
                labels.append(phrase['label'])

        return processed_text, labels
    
    def __len__(self):
        return len(self.processed_data)
    
    def __getitem__(self, index) :
        return self.processed_data[index]
        
                