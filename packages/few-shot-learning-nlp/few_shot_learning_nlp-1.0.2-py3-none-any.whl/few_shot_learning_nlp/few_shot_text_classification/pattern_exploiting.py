import torch
from torch.utils.data import Dataset, DataLoader
from transformers import AutoModelForMaskedLM, AutoTokenizer
from typing import Dict
from tqdm import tqdm
import logging
from torcheval.metrics.functional import multiclass_f1_score, multiclass_confusion_matrix, binary_f1_score
from torch.nn.functional import cross_entropy
from copy import deepcopy
from IPython.display import clear_output

class PETTrainer:
    """
    Trainer for fine-tuning a model with PET (Pattern Exploiting Training).

    Args:
        model (AutoModelForMaskedLM): The pretrained model to be fine-tuned.
        verbalizer (Dict): A dictionary mapping class labels to corresponding tokens.
        tokenizer (AutoTokenizer): The tokenizer associated with the model.
        num_classes (int): The number of classes in the classification task.
        device (str, optional): The device on which to perform calculations ('cuda' or 'cpu'). Defaults to 'cuda'.

    Attributes:
        model (AutoModelForMaskedLM): The pretrained model to be fine-tuned.
        verbalizer (Dict): A dictionary mapping class labels to corresponding tokens.
        tokenizer (AutoTokenizer): The tokenizer associated with the model.
        num_classes (int): The number of classes in the classification task.
        device (str): The device on which calculations are performed.

    Methods:
        get_y_true(input, inverse_verbalizer, device): Get the true labels from the input data.
        train(train_dataloader, val_dataloader, alpha, loss_fn, device, lr, n_epochs): Train the PET model.

    Notes:
        - PET (Pattern Exploiting Training) is a technique for fine-tuning pretrained language models for text classification tasks.
        - The trainer supports multi-class classification tasks.
        - The model is trained using a combination of Masked Language Model (MLM) loss and Cross-Entropy (CE) loss.
    """
    def __init__(
        self,
        model : AutoModelForMaskedLM,
        verbalizer : Dict,
        tokenizer : AutoTokenizer,
        num_classes : int,
        device : str = 'cuda',
    ) -> None:
        
        self.model = model
        self.verbalizer = verbalizer
        self.tokenizer = tokenizer
        self.num_classes = num_classes
        self.device = device
        self.inverse_verbalizer = {v:k for k, v in self.verbalizer.items()}

    @staticmethod
    def get_y_true(
        
        input: Dict[str, torch.Tensor],
        inverse_verbalizer : Dict,
        device : str = "cuda",
    )-> torch.tensor:
        
        """
        Get the true labels from the input data.

        Args:
            input (Dict[str, torch.Tensor]): The input data containing the true labels.
            inverse_verbalizer (Dict): A dictionary mapping tokens to class labels.
            device (str, optional): The device on which to perform calculations ('cuda' or 'cpu'). Defaults to 'cuda'.

        Returns:
            torch.Tensor: The true labels.

        Notes:
            - The true labels are extracted from the input data using the inverse verbalizer.
        """

        y_true = input['labels']
        y_true = y_true[y_true!= -100].item()
        y_true = inverse_verbalizer[y_true]
        
        return torch.tensor(y_true, device= device)

    def train(
        self,
        train_dataloader : DataLoader,
        val_dataloader : DataLoader,
        alpha : float,
        loss_fn : torch.nn.Module = None,
        device : str = 'cuda',
        lr : float = 1e-5,
        n_epochs : int =10,
    ):
        """
        Train the PET model.

        Args:
            train_dataloader (DataLoader): DataLoader for the training dataset.
            val_dataloader (DataLoader): DataLoader for the validation dataset.
            alpha (float): The weighting factor for balancing MLM and CE losses.
            loss_fn (torch.nn.Module, optional): Custom loss function for the CE loss. Defaults to None.
            device (str, optional): The device on which to perform calculations ('cuda' or 'cpu'). Defaults to 'cuda'.
            lr (float, optional): The learning rate for optimization. Defaults to 1e-5.
            n_epochs (int, optional): The number of training epochs. Defaults to 10.

        Returns:
            Tuple[List[float], torch.Tensor, AutoModelForMaskedLM]: A tuple containing training history, confusion matrix, and the best trained model.

        Notes:
            - The model is trained using the provided dataloaders and training parameters.
            - The training process combines Masked Language Model (MLM) loss and Cross-Entropy (CE) loss.
        """
        

        optimizer = torch.optim.Adam(
            self.model.parameters(),
            lr = lr
        )

        best_f1=  0
        self.best_model : AutoModelForMaskedLM = None
        confusion_matrix= None

        history = []

        for epoch in range(n_epochs):

            for input in train_dataloader:
                out = self.model(**input)

                loss_mlm = out['loss']

                y_true = PETTrainer.get_y_true(
                    input,
                    self.inverse_verbalizer
                )

                try:
                    mask_token_index = torch.where(input["input_ids"] == self.tokenizer.mask_token_id)[1]

                except:
                    mask_token_index = torch.where(input["input_ids"] == self.tokenizer.mask_token_id)[0]

                mask_token_logits = out.logits[0, mask_token_index, :]
                
                predictions = torch.Tensor(
                    [mask_token_logits[0,x] for x in self.verbalizer.values()]
                )

                probabilities = predictions.softmax(dim = 0).to(device)
                    
                loss_ce = cross_entropy(
                    probabilities,
                    y_true
                )

                if loss_fn is not None:
                    loss_ce = loss_fn(
                        probabilities, 
                        y_true
                    )
                loss_ce.requires_grad = True

                loss = alpha*loss_mlm + (1-alpha)* loss_ce
                

                loss.backward()
                optimizer.step()
                optimizer.zero_grad()

            with torch.no_grad():    
                y_true_val = torch.tensor([],device=device)
                y_pred_val = torch.tensor([],device=device)

                for input in tqdm(val_dataloader):
                    out = self.model(**input)

                    y_true = PETTrainer.get_y_true(
                        input,
                        self.inverse_verbalizer
                    )

                    try:
                        mask_token_index = torch.where(input["input_ids"] == self.tokenizer.mask_token_id)[1]

                    except:
                        mask_token_index = torch.where(input["input_ids"] == self.tokenizer.mask_token_id)[0]
                    mask_token_logits = out.logits[0, mask_token_index, :]
                    
                    predictions = torch.Tensor(
                        [mask_token_logits[0,x] for x in self.verbalizer.values()]
                    )

                    y_pred = predictions.argmax().to(device)

                    y_pred_val = torch.cat([
                        y_pred_val, 
                        torch.tensor([y_pred]).to(device)
                    ])

                    y_true_val = torch.cat([
                        y_true_val,
                        torch.tensor([y_true]).to(device)
                    ])
                
                if self.num_classes == 2:
                    f1 = binary_f1_score(
                        y_pred_val,
                        y_true_val,
                    )

                else:
                    f1 = multiclass_f1_score(
                        y_pred_val,
                        y_true_val,
                        num_classes=self.num_classes
                    )


                conf_matrix = multiclass_confusion_matrix(
                    y_pred_val.to(torch.int64),
                    y_true_val.to(torch.int64),
                    num_classes= self.num_classes
                )
                
                clear_output(True)
                print(f"Epoch {epoch}")
                print(f'f1-score : {f1.item()}')
                print(conf_matrix)
                


                if f1 > best_f1:
                    best_f1 = f1
                    confusion_matrix = conf_matrix
                    self.best_model = deepcopy(self.model)

                history.append(f1.item())

                logging.info("-------------------------")
                logging.info(f"End of epoch {epoch}: f1 = {f1.item()}")
                logging.info(conf_matrix)
        
        return history, confusion_matrix, self.best_model
    
    def test(
        self,
        test_dataloader : DataLoader,
    ):
        """
        Perform inference/testing using the trained model on the provided test data.

        Args:
            test_dataloader (DataLoader): DataLoader containing the test data.

        Returns:
            Tuple[torch.Tensor, torch.Tensor]: A tuple containing true labels and predicted labels.

        Notes:
            - This method performs inference/testing using the trained model on the provided test data.
            - It iterates through the test DataLoader, calculates predictions using the model,
            and collects true and predicted labels.
            - The method operates in evaluation mode (torch.no_grad() context).
            - The returned tensors contain true labels and corresponding predicted labels.
        """
        with torch.no_grad():
            y_true_test = torch.tensor([],device=self.device)
            y_pred_test = torch.tensor([],device=self.device)

            for input in tqdm(test_dataloader):
                out = self.best_model(**input)

                y_true = self.get_y_true(
                    input,
                    self.inverse_verbalizer
                )

                try:
                    mask_token_index = torch.where(input["input_ids"] == self.tokenizer.mask_token_id)[1]

                except:
                    mask_token_index = torch.where(input["input_ids"] == self.tokenizer.mask_token_id)[0]
                mask_token_logits = out.logits[0, mask_token_index, :]
                
                predictions = torch.Tensor(
                    [mask_token_logits[0,x] for x in self.verbalizer.values()]
                )

                y_pred = predictions.argmax().to(self.device)

                y_pred_test = torch.cat([
                    y_pred_test, 
                    torch.tensor([y_pred]).to(self.device)
                ])

                y_true_test = torch.cat([
                    y_true_test,
                    torch.tensor([y_true]).to(self.device)
                ])

            return y_true_test, y_pred_test
            