{
  "prog": {
    "lm": null,
    "traces": [],
    "train": [],
    "demos": [
      {
        "issue": "Read_csv should raise error on bad dtypes\nOPs code from http:\/\/stackoverflow.com\/questions\/16988526\/pandas-reading-csv-as-string-type\n\nAdd: auto conversion from a passed `str` or `np.string_` to `np.object` might be ok\n\n```\ndf\n           A         B\n1A  0.209059  0.275554\n1B  0.742666  0.721165\n\ndf.to_csv(savefile)\n\ndf_read = pd.read_csv(savefile, dtype=str, index_col=0)\n   A  B\nB  (  <\n```\n\nActually I get something different on dev:\n\n```\nIn [101]: pd.read_csv('a', index_col=0, dtype=str)\nOut[101]:\n A B\n\n\n```\n\n",
        "patch": "<patch>\ndiff --git a\/RELEASE.rst b\/RELEASE.rst\n--- a\/RELEASE.rst\n+++ b\/RELEASE.rst\n@@ -219,6 +219,7 @@ pandas 0.11.1\n   - Incorrectly read a HDFStore multi-index Frame witha column specification (GH3748_)\n   - ``read_html`` now correctly skips tests (GH3741_)\n   - Fix incorrect arguments passed to concat that are not list-like (e.g. concat(df1,df2)) (GH3481_)\n+  - Correctly parse when passed the ``dtype=str`` (or other variable-len string dtypes) in ``read_csv`` (GH3795_)\n \n .. _GH3164: https:\/\/github.com\/pydata\/pandas\/issues\/3164\n .. _GH2786: https:\/\/github.com\/pydata\/pandas\/issues\/2786\n@@ -307,6 +308,7 @@ pandas 0.11.1\n .. _GH3741: https:\/\/github.com\/pydata\/pandas\/issues\/3741\n .. _GH3750: https:\/\/github.com\/pydata\/pandas\/issues\/3750\n .. _GH3726: https:\/\/github.com\/pydata\/pandas\/issues\/3726\n+.. _GH3795: https:\/\/github.com\/pydata\/pandas\/issues\/3795\n \n pandas 0.11.0\n =============\ndiff --git a\/pandas\/parser.pyx b\/pandas\/parser.pyx\n--- a\/pandas\/parser.pyx\n+++ b\/pandas\/parser.pyx\n@@ -990,20 +990,36 @@ cdef class TextReader:\n                                              na_filter, na_hashset)\n             return result, na_count\n         elif dtype[1] == 'c':\n-            raise NotImplementedError\n+            raise NotImplementedError(\"the dtype %s is not supported for parsing\" % dtype)\n \n         elif dtype[1] == 'S':\n             # TODO: na handling\n             width = int(dtype[2:])\n-            result = _to_fw_string(self.parser, i, start, end, width)\n-            return result, 0\n+            if width > 0:\n+                result = _to_fw_string(self.parser, i, start, end, width)\n+                return result, 0\n+\n+            # treat as a regular string parsing\n+            return self._string_convert(i, start, end, na_filter,\n+                                       na_hashset)\n         elif dtype[1] == 'U':\n             width = int(dtype[2:])\n-            raise NotImplementedError\n+            if width > 0:\n+                raise NotImplementedError(\"the dtype %s is not supported for parsing\" % dtype)\n+\n+            # unicode variable width\n+            return self._string_convert(i, start, end, na_filter,\n+                                        na_hashset)\n+\n \n         elif dtype[1] == 'O':\n             return self._string_convert(i, start, end, na_filter,\n                                         na_hashset)\n+        else:\n+            if dtype[1] == 'M':\n+                 raise TypeError(\"the dtype %s is not supported for parsing, \"\n+                                 \"pass this column using parse_dates instead\" % dtype)\n+            raise TypeError(\"the dtype %s is not supported for parsing\" % dtype)\n \n     cdef _string_convert(self, Py_ssize_t i, int start, int end,\n                          bint na_filter, kh_str_t *na_hashset):\n\n<\/patch>",
        "test_patch": ""
      },
      {
        "issue": "Convert vendored `boltons` to a dependency\n### What is the idea?\r\n\r\n`boltons 21.0.0` is now available on `pkgs\/main` (and has been available on `conda-forge`). Deprecate the vendored `boltons` in favor of the latest available packages.\r\n\r\n### Why is this needed?\r\n\r\nNo need to vendor `boltons`.\r\n\r\n### What should happen?\r\n\r\nDeprecate vendored `boltons`.\r\n\n",
        "patch": "<patch>\ndiff --git a\/conda\/_vendor\/boltons\/__init__.py b\/conda\/_vendor\/boltons\/__init__.py\n--- a\/conda\/_vendor\/boltons\/__init__.py\n+++ b\/conda\/_vendor\/boltons\/__init__.py\n@@ -0,0 +1,2 @@\n+from ...deprecations import deprecated\n+deprecated.module(\"23.9\", \"24.3\", addendum=\"Use `boltons` instead.\")\ndiff --git a\/conda\/auxlib\/entity.py b\/conda\/auxlib\/entity.py\n--- a\/conda\/auxlib\/entity.py\n+++ b\/conda\/auxlib\/entity.py\n@@ -237,14 +237,17 @@\n \n from collections.abc import Mapping, Sequence\n from datetime import datetime\n+from enum import Enum\n from functools import reduce\n from json import JSONEncoder, dumps as json_dumps, loads as json_loads\n from logging import getLogger\n \n-from enum import Enum\n+try:\n+    from boltons.timeutils import isoparse\n+except ImportError:\n+    from .._vendor.boltons.timeutils import isoparse\n \n from . import NULL\n-from .._vendor.boltons.timeutils import isoparse\n from .._vendor.frozendict import frozendict\n from .collection import AttrDict, make_immutable\n from .compat import isiterable, odict\ndiff --git a\/conda\/base\/context.py b\/conda\/base\/context.py\n--- a\/conda\/base\/context.py\n+++ b\/conda\/base\/context.py\n@@ -12,6 +12,11 @@\n import struct\n from contextlib import contextmanager\n \n+try:\n+    from boltons.setutils import IndexedSet\n+except ImportError:\n+    from .._vendor.boltons.setutils import IndexedSet\n+\n from .constants import (\n     APP_NAME,\n     ChannelPriority,\n@@ -39,7 +44,6 @@\n from .._vendor.appdirs import user_data_dir\n from ..auxlib.decorators import memoizedproperty\n from ..auxlib.ish import dals\n-from .._vendor.boltons.setutils import IndexedSet\n from .._vendor.frozendict import frozendict\n from ..common.compat import NoneType, on_win\n from ..common.configuration import (Configuration, ConfigurationLoadError, MapParameter,\ndiff --git a\/conda\/cli\/main_search.py b\/conda\/cli\/main_search.py\n--- a\/conda\/cli\/main_search.py\n+++ b\/conda\/cli\/main_search.py\n@@ -2,9 +2,8 @@\n # SPDX-License-Identifier: BSD-3-Clause\n \n from collections import defaultdict\n-from datetime import datetime\n+from datetime import datetime, timezone\n \n-from .._vendor.boltons.timeutils import UTC\n from ..base.context import context\n from ..cli.common import stdout_json\n from ..common.io import Spinner\n@@ -134,7 +133,9 @@ def push_line(display_name, attr_name):\n     push_line(\"url\", \"url\")\n     push_line(\"md5\", \"md5\")\n     if record.timestamp:\n-        date_str = datetime.fromtimestamp(record.timestamp, UTC).strftime('%Y-%m-%d %H:%M:%S %Z')\n+        date_str = datetime.fromtimestamp(record.timestamp, timezone.utc).strftime(\n+            \"%Y-%m-%d %H:%M:%S %Z\"\n+        )\n         builder.append(\"%-12s: %s\" % (\"timestamp\", date_str))\n     if record.track_features:\n         builder.append(\"%-12s: %s\" % (\"track_features\", dashlist(record.track_features)))\ndiff --git a\/conda\/common\/configuration.py b\/conda\/common\/configuration.py\n--- a\/conda\/common\/configuration.py\n+++ b\/conda\/common\/configuration.py\n@@ -30,6 +30,11 @@\n if TYPE_CHECKING:\n     from typing import Sequence\n \n+try:\n+    from boltons.setutils import IndexedSet\n+except ImportError:\n+    from .._vendor.boltons.setutils import IndexedSet\n+\n from .compat import isiterable, primitive_types\n from .constants import NULL\n from .path import expand\n@@ -40,7 +45,6 @@\n from ..auxlib.type_coercion import TypeCoercionError, typify, typify_data_structure\n from ..common.iterators import unique\n from .._vendor.frozendict import frozendict\n-from .._vendor.boltons.setutils import IndexedSet\n \n try:\n     from ruamel.yaml.comments import CommentedSeq, CommentedMap\ndiff --git a\/conda\/core\/index.py b\/conda\/core\/index.py\n--- a\/conda\/core\/index.py\n+++ b\/conda\/core\/index.py\n@@ -6,11 +6,15 @@\n import platform\n import sys\n \n+try:\n+    from boltons.setutils import IndexedSet\n+except ImportError:\n+    from .._vendor.boltons.setutils import IndexedSet\n+\n from ..deprecations import deprecated\n from .package_cache_data import PackageCacheData\n from .prefix_data import PrefixData\n from .subdir_data import SubdirData, make_feature_record\n-from .._vendor.boltons.setutils import IndexedSet\n from ..base.context import context\n from ..common.io import ThreadLimitedThreadPoolExecutor, time_recorder\n from ..exceptions import ChannelNotAllowed, InvalidSpec, PluginError\ndiff --git a\/conda\/core\/solve.py b\/conda\/core\/solve.py\n--- a\/conda\/core\/solve.py\n+++ b\/conda\/core\/solve.py\n@@ -9,6 +9,11 @@\n import sys\n from textwrap import dedent\n \n+try:\n+    from boltons.setutils import IndexedSet\n+except ImportError:\n+    from .._vendor.boltons.setutils import IndexedSet\n+\n from conda.common.iterators import groupby_to_dict as groupby\n \n from .index import get_reduced_index, _supplement_index_with_system\n@@ -19,7 +24,6 @@\n from ..deprecations import deprecated\n from ..auxlib.decorators import memoizedproperty\n from ..auxlib.ish import dals\n-from .._vendor.boltons.setutils import IndexedSet\n from ..base.constants import DepsModifier, UNKNOWN_CHANNEL, UpdateModifier, REPODATA_FN\n from ..base.context import context\n from ..common.constants import NULL\ndiff --git a\/conda\/core\/subdir_data.py b\/conda\/core\/subdir_data.py\n--- a\/conda\/core\/subdir_data.py\n+++ b\/conda\/core\/subdir_data.py\n@@ -13,6 +13,7 @@\n from contextlib import closing\n from errno import EACCES, ENODEV, EPERM, EROFS\n from functools import partial\n+from genericpath import getmtime, isfile\n from io import open as io_open\n from itertools import chain, islice\n from logging import getLogger\n@@ -20,23 +21,18 @@\n from os.path import dirname, exists, isdir, join, splitext\n from time import time\n \n-from genericpath import getmtime, isfile\n-\n-from conda.common.iterators import groupby_to_dict as groupby\n-from conda.gateways.repodata import (\n-    CondaRepoInterface,\n-    RepodataIsEmpty,\n-    RepoInterface,\n-    Response304ContentUnchanged,\n-)\n+try:\n+    from boltons.setutils import IndexedSet\n+except ImportError:\n+    from .._vendor.boltons.setutils import IndexedSet\n \n from .. import CondaError\n-from .._vendor.boltons.setutils import IndexedSet\n from ..auxlib.ish import dals\n from ..base.constants import CONDA_PACKAGE_EXTENSION_V1, REPODATA_FN\n from ..base.context import context\n from ..common.compat import ensure_binary, ensure_unicode\n from ..common.io import DummyExecutor, ThreadLimitedThreadPoolExecutor, dashlist\n+from ..common.iterators import groupby_to_dict as groupby\n from ..common.path import url_to_path\n from ..common.url import join_url\n from ..core.package_cache_data import PackageCacheData\n@@ -44,6 +40,12 @@\n from ..gateways.disk import mkdir_p, mkdir_p_sudo_safe\n from ..gateways.disk.delete import rm_rf\n from ..gateways.disk.update import touch\n+from ..gateways.repodata import (\n+    CondaRepoInterface,\n+    RepodataIsEmpty,\n+    RepoInterface,\n+    Response304ContentUnchanged,\n+)\n from ..models.channel import Channel, all_channel_urls\n from ..models.match_spec import MatchSpec\n from ..models.records import PackageRecord\ndiff --git a\/conda\/models\/channel.py b\/conda\/models\/channel.py\n--- a\/conda\/models\/channel.py\n+++ b\/conda\/models\/channel.py\n@@ -5,7 +5,11 @@\n from itertools import chain\n from logging import getLogger\n \n-from .._vendor.boltons.setutils import IndexedSet\n+try:\n+    from boltons.setutils import IndexedSet\n+except ImportError:\n+    from .._vendor.boltons.setutils import IndexedSet\n+\n from ..base.constants import DEFAULTS_CHANNEL_NAME, MAX_CHANNEL_PRIORITY, UNKNOWN_CHANNEL\n from ..base.context import context, Context\n from ..common.compat import ensure_text_type, isiterable\ndiff --git a\/conda\/models\/prefix_graph.py b\/conda\/models\/prefix_graph.py\n--- a\/conda\/models\/prefix_graph.py\n+++ b\/conda\/models\/prefix_graph.py\n@@ -3,9 +3,13 @@\n from collections import defaultdict\n from logging import getLogger\n \n+try:\n+    from boltons.setutils import IndexedSet\n+except ImportError:\n+    from .._vendor.boltons.setutils import IndexedSet\n+\n from .enums import NoarchType\n from .match_spec import MatchSpec\n-from .._vendor.boltons.setutils import IndexedSet\n from ..base.context import context\n from ..common.compat import on_win\n from ..exceptions import CyclicalDependencyError\ndiff --git a\/conda\/models\/records.py b\/conda\/models\/records.py\n--- a\/conda\/models\/records.py\n+++ b\/conda\/models\/records.py\n@@ -13,6 +13,11 @@\n \n from os.path import basename, join\n \n+try:\n+    from boltons.timeutils import dt_to_timestamp, isoparse\n+except ImportError:\n+    from .._vendor.boltons.timeutils import dt_to_timestamp, isoparse\n+\n from .channel import Channel\n from .enums import FileMode, LinkType, NoarchType, PackageType, PathType, Platform\n from .match_spec import MatchSpec\n@@ -27,7 +32,6 @@\n     NumberField,\n     StringField,\n )\n-from .._vendor.boltons.timeutils import dt_to_timestamp, isoparse\n from ..base.context import context\n from ..common.compat import isiterable\n from ..exceptions import PathNotFoundError\ndiff --git a\/conda\/plan.py b\/conda\/plan.py\n--- a\/conda\/plan.py\n+++ b\/conda\/plan.py\n@@ -14,7 +14,11 @@\n from logging import getLogger\n import sys\n \n-from ._vendor.boltons.setutils import IndexedSet\n+try:\n+    from boltons.setutils import IndexedSet\n+except ImportError:\n+    from ._vendor.boltons.setutils import IndexedSet\n+\n from .base.constants import DEFAULTS_CHANNEL_NAME, UNKNOWN_CHANNEL\n from .base.context import context, stack_context_default\n from .common.io import dashlist, env_vars, time_recorder\n@@ -440,7 +444,6 @@ def install_actions(prefix, index, specs, force=False, only_names=None, always_c\n         'CONDA_SOLVER_IGNORE_TIMESTAMPS': 'false',\n     }, stack_callback=stack_context_default):\n         from os.path import basename\n-        from ._vendor.boltons.setutils import IndexedSet\n         from .models.channel import Channel\n         from .models.dist import Dist\n         if channel_priority_map:\ndiff --git a\/conda_env\/installers\/conda.py b\/conda_env\/installers\/conda.py\n--- a\/conda_env\/installers\/conda.py\n+++ b\/conda_env\/installers\/conda.py\n@@ -4,7 +4,11 @@\n import tempfile\n from os.path import basename\n \n-from conda._vendor.boltons.setutils import IndexedSet\n+try:\n+    from boltons.setutils import IndexedSet\n+except ImportError:\n+    from conda._vendor.boltons.setutils import IndexedSet\n+\n from conda.base.constants import UpdateModifier\n from conda.base.context import context\n from conda.common.constants import NULL\n\n<\/patch>",
        "test_patch": ""
      },
      {
        "issue": "Feature request: add median, mode & number of unique entries to pandas.DataFrame.describe()\nWould come in handy for a number of a different applications from basic statistics, to understanding one's data, to estimating machine learning algorithmic load.\n\ndupe of #2749\n\n",
        "patch": "<patch>\ndiff --git a\/doc\/source\/api.rst b\/doc\/source\/api.rst\n--- a\/doc\/source\/api.rst\n+++ b\/doc\/source\/api.rst\n@@ -314,6 +314,8 @@ Function application, GroupBy & Window\n    :toctree: generated\/\n \n    Series.apply\n+   Series.aggregate\n+   Series.transform\n    Series.map\n    Series.groupby\n    Series.rolling\n@@ -831,6 +833,8 @@ Function application, GroupBy & Window\n \n    DataFrame.apply\n    DataFrame.applymap\n+   DataFrame.aggregate\n+   DataFrame.transform\n    DataFrame.groupby\n    DataFrame.rolling\n    DataFrame.expanding\ndiff --git a\/doc\/source\/basics.rst b\/doc\/source\/basics.rst\n--- a\/doc\/source\/basics.rst\n+++ b\/doc\/source\/basics.rst\n@@ -702,7 +702,8 @@ on an entire ``DataFrame`` or ``Series``, row- or column-wise, or elementwise.\n \n 1. `Tablewise Function Application`_: :meth:`~DataFrame.pipe`\n 2. `Row or Column-wise Function Application`_: :meth:`~DataFrame.apply`\n-3. Elementwise_ function application: :meth:`~DataFrame.applymap`\n+3. `Aggregation API`_: :meth:`~DataFrame.agg` and :meth:`~DataFrame.transform`\n+4. `Applying Elementwise Functions`_: :meth:`~DataFrame.applymap`\n \n .. _basics.pipe:\n \n@@ -778,6 +779,13 @@ statistics methods, take an optional ``axis`` argument:\n    df.apply(np.cumsum)\n    df.apply(np.exp)\n \n+``.apply()`` will also dispatch on a string method name.\n+\n+.. ipython:: python\n+\n+   df.apply('mean')\n+   df.apply('mean', axis=1)\n+\n Depending on the return type of the function passed to :meth:`~DataFrame.apply`,\n the result will either be of lower dimension or the same dimension.\n \n@@ -827,16 +835,223 @@ set to True, the passed function will instead receive an ndarray object, which\n has positive performance implications if you do not need the indexing\n functionality.\n \n-.. seealso::\n+.. _basics.aggregate:\n+\n+Aggregation API\n+~~~~~~~~~~~~~~~\n+\n+.. versionadded:: 0.20.0\n+\n+The aggregation API allows one to express possibly multiple aggregation operations in a single concise way.\n+This API is similar across pandas objects, see :ref:`groupby API <groupby.aggregate>`, the\n+:ref:`window functions API <stats.aggregate>`, and the :ref:`resample API <timeseries.aggregate>`.\n+The entry point for aggregation is the method :meth:`~DataFrame.aggregate`, or the alias :meth:`~DataFrame.agg`.\n+\n+We will use a similar starting frame from above:\n+\n+.. ipython:: python\n+\n+   tsdf = pd.DataFrame(np.random.randn(10, 3), columns=['A', 'B', 'C'],\n+                       index=pd.date_range('1\/1\/2000', periods=10))\n+   tsdf.iloc[3:7] = np.nan\n+   tsdf\n+\n+Using a single function is equivalent to :meth:`~DataFrame.apply`; You can also pass named methods as strings.\n+These will return a ``Series`` of the aggregated output:\n+\n+.. ipython:: python\n+\n+   tsdf.agg(np.sum)\n+\n+   tsdf.agg('sum')\n+\n+   # these are equivalent to a ``.sum()`` because we are aggregating on a single function\n+   tsdf.sum()\n+\n+Single aggregations on a ``Series`` this will result in a scalar value:\n+\n+.. ipython:: python\n+\n+   tsdf.A.agg('sum')\n+\n+\n+Aggregating with multiple functions\n++++++++++++++++++++++++++++++++++++\n+\n+You can pass multiple aggregation arguments as a list.\n+The results of each of the passed functions will be a row in the resultant ``DataFrame``.\n+These are naturally named from the aggregation function.\n+\n+.. ipython:: python\n+\n+   tsdf.agg(['sum'])\n+\n+Multiple functions yield multiple rows:\n+\n+.. ipython:: python\n+\n+   tsdf.agg(['sum', 'mean'])\n+\n+On a ``Series``, multiple functions return a ``Series``, indexed by the function names:\n+\n+.. ipython:: python\n+\n+   tsdf.A.agg(['sum', 'mean'])\n+\n+Passing a ``lambda`` function will yield a ``<lambda>`` named row:\n+\n+.. ipython:: python\n+\n+   tsdf.A.agg(['sum', lambda x: x.mean()])\n+\n+Passing a named function will yield that name for the row:\n+\n+.. ipython:: python\n+\n+   def mymean(x):\n+      return x.mean()\n+\n+   tsdf.A.agg(['sum', mymean])\n+\n+Aggregating with a dict\n++++++++++++++++++++++++\n+\n+Passing a dictionary of column names to a scalar or a list of scalars, to ``DataFame.agg``\n+allows you to customize which functions are applied to which columns.\n+\n+.. ipython:: python\n+\n+   tsdf.agg({'A': 'mean', 'B': 'sum'})\n+\n+Passing a list-like will generate a ``DataFrame`` output. You will get a matrix-like output\n+of all of the aggregators. The output will consist of all unique functions. Those that are\n+not noted for a particular column will be ``NaN``:\n+\n+.. ipython:: python\n+\n+   tsdf.agg({'A': ['mean', 'min'], 'B': 'sum'})\n+\n+.. _basics.aggregation.mixed_dtypes:\n+\n+Mixed Dtypes\n+++++++++++++\n \n-   The section on :ref:`GroupBy <groupby>` demonstrates related, flexible\n-   functionality for grouping by some criterion, applying, and combining the\n-   results into a Series, DataFrame, etc.\n+When presented with mixed dtypes that cannot aggregate, ``.agg`` will only take the valid\n+aggregations. This is similiar to how groupby ``.agg`` works.\n \n-.. _Elementwise:\n+.. ipython:: python\n+\n+   mdf = pd.DataFrame({'A': [1, 2, 3],\n+                       'B': [1., 2., 3.],\n+                       'C': ['foo', 'bar', 'baz'],\n+                       'D': pd.date_range('20130101', periods=3)})\n+   mdf.dtypes\n+\n+.. ipython:: python\n+\n+   mdf.agg(['min', 'sum'])\n+\n+.. _basics.aggregation.custom_describe:\n+\n+Custom describe\n++++++++++++++++\n+\n+With ``.agg()`` is it possible to easily create a custom describe function, similar\n+to the built in :ref:`describe function <basics.describe>`.\n+\n+.. ipython:: python\n+\n+   from functools import partial\n+\n+   q_25 = partial(pd.Series.quantile, q=0.25)\n+   q_25.__name__ = '25%'\n+   q_75 = partial(pd.Series.quantile, q=0.75)\n+   q_75.__name__ = '75%'\n+\n+   tsdf.agg(['count', 'mean', 'std', 'min', q_25, 'median', q_75, 'max'])\n+\n+.. _basics.transform:\n+\n+Transform API\n+~~~~~~~~~~~~~\n \n-Applying elementwise Python functions\n-~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n+.. versionadded:: 0.20.0\n+\n+The :method:`~DataFrame.transform` method returns an object that is indexed the same (same size)\n+as the original. This API allows you to provide *multiple* operations at the same\n+time rather than one-by-one. Its api is quite similar to the ``.agg`` API.\n+\n+Use a similar frame to the above sections.\n+\n+.. ipython:: python\n+\n+   tsdf = pd.DataFrame(np.random.randn(10, 3), columns=['A', 'B', 'C'],\n+                       index=pd.date_range('1\/1\/2000', periods=10))\n+   tsdf.iloc[3:7] = np.nan\n+   tsdf\n+\n+Transform the entire frame. Transform allows functions to input as a numpy function, string\n+function name and user defined function.\n+\n+.. ipython:: python\n+\n+   tsdf.transform(np.abs)\n+   tsdf.transform('abs')\n+   tsdf.transform(lambda x: x.abs())\n+\n+Since this is a single function, this is equivalent to a ufunc application\n+\n+.. ipython:: python\n+\n+   np.abs(tsdf)\n+\n+Passing a single function to ``.transform()`` with a Series will yield a single Series in return.\n+\n+.. ipython:: python\n+\n+   tsdf.A.transform(np.abs)\n+\n+\n+Transform with multiple functions\n++++++++++++++++++++++++++++++++++\n+\n+Passing multiple functions will yield a column multi-indexed DataFrame.\n+The first level will be the original frame column names; the second level\n+will be the names of the transforming functions.\n+\n+.. ipython:: python\n+\n+   tsdf.transform([np.abs, lambda x: x+1])\n+\n+Passing multiple functions to a Series will yield a DataFrame. The\n+resulting column names will be the transforming functions.\n+\n+.. ipython:: python\n+\n+   tsdf.A.transform([np.abs, lambda x: x+1])\n+\n+\n+Transforming with a dict\n+++++++++++++++++++++++++\n+\n+\n+Passing a dict of functions will will allow selective transforming per column.\n+\n+.. ipython:: python\n+\n+   tsdf.transform({'A': np.abs, 'B': lambda x: x+1})\n+\n+Passing a dict of lists will generate a multi-indexed DataFrame with these\n+selective transforms.\n+\n+.. ipython:: python\n+\n+   tsdf.transform({'A': np.abs, 'B': [lambda x: x+1, 'sqrt']})\n+\n+.. _basics.elementwise:\n+\n+Applying Elementwise Functions\n+~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n \n Since not all functions can be vectorized (accept NumPy arrays and return\n another array or value), the methods :meth:`~DataFrame.applymap` on DataFrame\ndiff --git a\/doc\/source\/computation.rst b\/doc\/source\/computation.rst\n--- a\/doc\/source\/computation.rst\n+++ b\/doc\/source\/computation.rst\n@@ -617,7 +617,9 @@ Aggregation\n -----------\n \n Once the ``Rolling``, ``Expanding`` or ``EWM`` objects have been created, several methods are available to\n-perform multiple computations on the data. This is very similar to a ``.groupby(...).agg`` seen :ref:`here <groupby.aggregate>`.\n+perform multiple computations on the data. These operations are similar to the :ref:`aggregating API <basics.aggregate>`,\n+:ref:`groupby aggregates <groupby.aggregate>`, and :ref:`resample API <timeseries.aggregate>`.\n+\n \n .. ipython:: python\n \n@@ -642,10 +644,10 @@ columns if none are selected.\n \n .. _stats.aggregate.multifunc:\n \n-Applying multiple functions at once\n-~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n+Applying multiple functions\n+~~~~~~~~~~~~~~~~~~~~~~~~~~~\n \n-With windowed Series you can also pass a list or dict of functions to do\n+With windowed ``Series`` you can also pass a list of functions to do\n aggregation with, outputting a DataFrame:\n \n .. ipython:: python\n@@ -666,7 +668,7 @@ Applying different functions to DataFrame columns\n ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n \n By passing a dict to ``aggregate`` you can apply a different aggregation to the\n-columns of a DataFrame:\n+columns of a ``DataFrame``:\n \n .. ipython:: python\n    :okexcept:\ndiff --git a\/doc\/source\/groupby.rst b\/doc\/source\/groupby.rst\n--- a\/doc\/source\/groupby.rst\n+++ b\/doc\/source\/groupby.rst\n@@ -439,7 +439,9 @@ Aggregation\n -----------\n \n Once the GroupBy object has been created, several methods are available to\n-perform a computation on the grouped data.\n+perform a computation on the grouped data. These operations are similar to the\n+:ref:`aggregating API <basics.aggregate>`, :ref:`window functions API <stats.aggregate>`,\n+and :ref:`resample API <timeseries.aggregate>`.\n \n An obvious one is aggregation via the ``aggregate`` or equivalently ``agg`` method:\n \ndiff --git a\/doc\/source\/timeseries.rst b\/doc\/source\/timeseries.rst\n--- a\/doc\/source\/timeseries.rst\n+++ b\/doc\/source\/timeseries.rst\n@@ -1519,11 +1519,13 @@ We can instead only resample those groups where we have points as follows:\n \n     ts.groupby(partial(round, freq='3T')).sum()\n \n+.. _timeseries.aggregate:\n+\n Aggregation\n ~~~~~~~~~~~\n \n-Similar to :ref:`groupby aggregates <groupby.aggregate>` and the :ref:`window functions <stats.aggregate>`, a ``Resampler`` can be selectively\n-resampled.\n+Similar to the :ref:`aggregating API <basics.aggregate>`, :ref:`groupby aggregates API <groupby.aggregate>`, and  the :ref:`window functions API <stats.aggregate>`,\n+a ``Resampler`` can be selectively resampled.\n \n Resampling a ``DataFrame``, the default will be to act on all columns with the same function.\n \ndiff --git a\/doc\/source\/whatsnew\/v0.20.0.txt b\/doc\/source\/whatsnew\/v0.20.0.txt\n--- a\/doc\/source\/whatsnew\/v0.20.0.txt\n+++ b\/doc\/source\/whatsnew\/v0.20.0.txt\n@@ -9,6 +9,8 @@ users upgrade to this version.\n \n Highlights include:\n \n+- new ``.agg()`` API for Series\/DataFrame similar to the groupby-rolling-resample API's, see :ref:`here <whatsnew_0200.enhancements.agg>`\n+- Integration with the ``feather-format``, including a new top-level ``pd.read_feather()`` and ``DataFrame.to_feather()`` method, see :ref:`here <io.feather>`.\n - The ``.ix`` indexer has been deprecated, see :ref:`here <whatsnew_0200.api_breaking.deprecate_ix>`\n - ``Panel`` has been deprecated, see :ref:`here <whatsnew_0200.api_breaking.deprecate_panel>`\n - Improved user API when accessing levels in ``.groupby()``, see :ref:`here <whatsnew_0200.enhancements.groupby_access>`\n@@ -32,6 +34,68 @@ Check the :ref:`API Changes <whatsnew_0200.api_breaking>` and :ref:`deprecations\n New features\n ~~~~~~~~~~~~\n \n+.. _whatsnew_0200.enhancements.agg:\n+\n+``agg`` API\n+^^^^^^^^^^^\n+\n+Series & DataFrame have been enhanced to support the aggregation API. This is an already familiar API that\n+is supported for groupby, window operations, and resampling. This allows one to express, possibly multiple\n+aggregation operations, in a single concise way by using :meth:`~DataFrame.agg`,\n+and :meth:`~DataFrame.transform`. The full documentation is :ref:`here <basics.aggregate>`` (:issue:`1623`)\n+\n+Here is a sample\n+\n+.. ipython:: python\n+\n+   df = pd.DataFrame(np.random.randn(10, 3), columns=['A', 'B', 'C'],\n+                    index=pd.date_range('1\/1\/2000', periods=10))\n+   df.iloc[3:7] = np.nan\n+   df\n+\n+One can operate using string function names, callables, lists, or dictionaries of these.\n+\n+Using a single function is equivalent to ``.apply``.\n+\n+.. ipython:: python\n+\n+   df.agg('sum')\n+\n+Multiple functions in lists.\n+\n+.. ipython:: python\n+\n+   df.agg(['sum', 'min'])\n+\n+Dictionaries to provide the ability to provide selective aggregation per column.\n+You will get a matrix-like output of all of the aggregators. The output will consist\n+of all unique functions. Those that are not noted for a particular column will be ``NaN``:\n+\n+.. ipython:: python\n+\n+   df.agg({'A' : ['sum', 'min'], 'B' : ['min', 'max']})\n+\n+The API also supports a ``.transform()`` function to provide for broadcasting results.\n+\n+.. ipython:: python\n+\n+   df.transform(['abs', lambda x: x - x.min()])\n+\n+When presented with mixed dtypes that cannot aggregate, ``.agg()`` will only take the valid\n+aggregations. This is similiar to how groupby ``.agg()`` works. (:issue:`15015`)\n+\n+.. ipython:: python\n+\n+   df = pd.DataFrame({'A': [1, 2, 3],\n+                      'B': [1., 2., 3.],\n+                      'C': ['foo', 'bar', 'baz'],\n+                      'D': pd.date_range('20130101', periods=3)})\n+   df.dtypes\n+\n+.. ipython:: python\n+\n+   df.agg(['min', 'sum'])\n+\n .. _whatsnew_0200.enhancements.dataio_dtype:\n \n ``dtype`` keyword for data IO\ndiff --git a\/pandas\/core\/base.py b\/pandas\/core\/base.py\n--- a\/pandas\/core\/base.py\n+++ b\/pandas\/core\/base.py\n@@ -470,6 +470,15 @@ def _aggregate(self, arg, *args, **kwargs):\n \n             obj = self._selected_obj\n \n+            def nested_renaming_depr(level=4):\n+                # deprecation of nested renaming\n+                # GH 15931\n+                warnings.warn(\n+                    (\"using a dict with renaming \"\n+                     \"is deprecated and will be removed in a future \"\n+                     \"version\"),\n+                    FutureWarning, stacklevel=level)\n+\n             # if we have a dict of any non-scalars\n             # eg. {'A' : ['mean']}, normalize all to\n             # be list-likes\n@@ -498,14 +507,10 @@ def _aggregate(self, arg, *args, **kwargs):\n                             raise SpecificationError('cannot perform renaming '\n                                                      'for {0} with a nested '\n                                                      'dictionary'.format(k))\n+                        nested_renaming_depr(4 + (_level or 0))\n \n-                        # deprecation of nested renaming\n-                        # GH 15931\n-                        warnings.warn(\n-                            (\"using a dict with renaming \"\n-                             \"is deprecated and will be removed in a future \"\n-                             \"version\"),\n-                            FutureWarning, stacklevel=4)\n+                    elif isinstance(obj, ABCSeries):\n+                        nested_renaming_depr()\n \n                 arg = new_arg\n \n@@ -515,11 +520,7 @@ def _aggregate(self, arg, *args, **kwargs):\n                 keys = list(compat.iterkeys(arg))\n                 if (isinstance(obj, ABCDataFrame) and\n                         len(obj.columns.intersection(keys)) != len(keys)):\n-                    warnings.warn(\n-                        (\"using a dict with renaming \"\n-                         \"is deprecated and will be removed in a future \"\n-                         \"version\"),\n-                        FutureWarning, stacklevel=4)\n+                    nested_renaming_depr()\n \n             from pandas.tools.concat import concat\n \ndiff --git a\/pandas\/core\/frame.py b\/pandas\/core\/frame.py\n--- a\/pandas\/core\/frame.py\n+++ b\/pandas\/core\/frame.py\n@@ -4189,6 +4189,42 @@ def diff(self, periods=1, axis=0):\n     # ----------------------------------------------------------------------\n     # Function application\n \n+    def _gotitem(self, key, ndim, subset=None):\n+        \"\"\"\n+        sub-classes to define\n+        return a sliced object\n+\n+        Parameters\n+        ----------\n+        key : string \/ list of selections\n+        ndim : 1,2\n+            requested ndim of result\n+        subset : object, default None\n+            subset to act on\n+        \"\"\"\n+        if subset is None:\n+            subset = self\n+\n+        # TODO: _shallow_copy(subset)?\n+        return self[key]\n+\n+    @Appender(_shared_docs['aggregate'] % _shared_doc_kwargs)\n+    def aggregate(self, func, axis=0, *args, **kwargs):\n+        axis = self._get_axis_number(axis)\n+\n+        # TODO: flipped axis\n+        result = None\n+        if axis == 0:\n+            try:\n+                result, how = self._aggregate(func, axis=0, *args, **kwargs)\n+            except TypeError:\n+                pass\n+        if result is None:\n+            return self.apply(func, axis=axis, args=args, **kwargs)\n+        return result\n+\n+    agg = aggregate\n+\n     def apply(self, func, axis=0, broadcast=False, raw=False, reduce=None,\n               args=(), **kwds):\n         \"\"\"\n@@ -4244,22 +4280,35 @@ def apply(self, func, axis=0, broadcast=False, raw=False, reduce=None,\n         See also\n         --------\n         DataFrame.applymap: For elementwise operations\n+        DataFrame.agg: only perform aggregating type operations\n+        DataFrame.transform: only perform transformating type operations\n \n         Returns\n         -------\n         applied : Series or DataFrame\n         \"\"\"\n         axis = self._get_axis_number(axis)\n-        if kwds or args and not isinstance(func, np.ufunc):\n+        ignore_failures = kwds.pop('ignore_failures', False)\n+\n+        # dispatch to agg\n+        if axis == 0 and isinstance(func, (list, dict)):\n+            return self.aggregate(func, axis=axis, *args, **kwds)\n+\n+        if len(self.columns) == 0 and len(self.index) == 0:\n+            return self._apply_empty_result(func, axis, reduce, *args, **kwds)\n \n+        # if we are a string, try to dispatch\n+        if isinstance(func, compat.string_types):\n+            if axis:\n+                kwds['axis'] = axis\n+            return getattr(self, func)(*args, **kwds)\n+\n+        if kwds or args and not isinstance(func, np.ufunc):\n             def f(x):\n                 return func(x, *args, **kwds)\n         else:\n             f = func\n \n-        if len(self.columns) == 0 and len(self.index) == 0:\n-            return self._apply_empty_result(func, axis, reduce, *args, **kwds)\n-\n         if isinstance(f, np.ufunc):\n             with np.errstate(all='ignore'):\n                 results = f(self.values)\n@@ -4276,7 +4325,10 @@ def f(x):\n                 else:\n                     if reduce is None:\n                         reduce = True\n-                    return self._apply_standard(f, axis, reduce=reduce)\n+                    return self._apply_standard(\n+                        f, axis,\n+                        reduce=reduce,\n+                        ignore_failures=ignore_failures)\n             else:\n                 return self._apply_broadcast(f, axis)\n \n@@ -5085,7 +5137,13 @@ def f(x):\n                         # this can end up with a non-reduction\n                         # but not always. if the types are mixed\n                         # with datelike then need to make sure a series\n-                        result = self.apply(f, reduce=False)\n+\n+                        # we only end up here if we have not specified\n+                        # numeric_only and yet we have tried a\n+                        # column-by-column reduction, where we have mixed type.\n+                        # So let's just do what we can\n+                        result = self.apply(f, reduce=False,\n+                                            ignore_failures=True)\n                         if result.ndim == self.ndim:\n                             result = result.iloc[0]\n                         return result\ndiff --git a\/pandas\/core\/generic.py b\/pandas\/core\/generic.py\n--- a\/pandas\/core\/generic.py\n+++ b\/pandas\/core\/generic.py\n@@ -32,7 +32,7 @@\n                                 SettingWithCopyError, SettingWithCopyWarning,\n                                 AbstractMethodError)\n \n-from pandas.core.base import PandasObject\n+from pandas.core.base import PandasObject, SelectionMixin\n from pandas.core.index import (Index, MultiIndex, _ensure_index,\n                                InvalidIndexError)\n import pandas.core.indexing as indexing\n@@ -91,7 +91,7 @@ def _single_replace(self, to_replace, method, inplace, limit):\n     return result\n \n \n-class NDFrame(PandasObject):\n+class NDFrame(PandasObject, SelectionMixin):\n     \"\"\"\n     N-dimensional analogue of DataFrame. Store multi-dimensional in a\n     size-mutable, labeled data structure\n@@ -459,6 +459,16 @@ def size(self):\n         \"\"\"number of elements in the NDFrame\"\"\"\n         return np.prod(self.shape)\n \n+    @property\n+    def _selected_obj(self):\n+        \"\"\" internal compat with SelectionMixin \"\"\"\n+        return self\n+\n+    @property\n+    def _obj_with_exclusions(self):\n+        \"\"\" internal compat with SelectionMixin \"\"\"\n+        return self\n+\n     def _expand_axes(self, key):\n         new_axes = []\n         for k, ax in zip(key, self.axes):\n@@ -2853,6 +2863,66 @@ def pipe(self, func, *args, **kwargs):\n         else:\n             return func(self, *args, **kwargs)\n \n+    _shared_docs['aggregate'] = (\"\"\"\n+    Aggregate using input function or dict of {column ->\n+    function}\n+\n+    .. versionadded:: 0.20.0\n+\n+    Parameters\n+    ----------\n+    func : callable, string, dictionary, or list of string\/callables\n+        Function to use for aggregating the data. If a function, must either\n+        work when passed a DataFrame or when passed to DataFrame.apply. If\n+        passed a dict, the keys must be DataFrame column names.\n+\n+        Accepted Combinations are:\n+        - string function name\n+        - function\n+        - list of functions\n+        - dict of column names -> functions (or list of functions)\n+\n+    Notes\n+    -----\n+    Numpy functions mean\/median\/prod\/sum\/std\/var are special cased so the\n+    default behavior is applying the function along axis=0\n+    (e.g., np.mean(arr_2d, axis=0)) as opposed to\n+    mimicking the default Numpy behavior (e.g., np.mean(arr_2d)).\n+\n+    Returns\n+    -------\n+    aggregated : %(klass)s\n+\n+    See also\n+    --------\n+    \"\"\")\n+\n+    _shared_docs['transform'] = (\"\"\"\n+    Call function producing a like-indexed %(klass)s\n+    and return a %(klass)s with the transformed values`\n+\n+    .. versionadded:: 0.20.0\n+\n+    Parameters\n+    ----------\n+    func : callable, string, dictionary, or list of string\/callables\n+        To apply to column\n+\n+        Accepted Combinations are:\n+        - string function name\n+        - function\n+        - list of functions\n+        - dict of column names -> functions (or list of functions)\n+\n+    Examples\n+    --------\n+    >>> df.transform(lambda x: (x - x.mean()) \/ x.std())\n+\n+    Returns\n+    -------\n+    transformed : %(klass)s\n+    \"\"\")\n+\n     # ----------------------------------------------------------------------\n     # Attribute access\n \n@@ -5990,6 +6060,17 @@ def ewm(self, com=None, span=None, halflife=None, alpha=None,\n \n         cls.ewm = ewm\n \n+        @Appender(_shared_docs['transform'] % _shared_doc_kwargs)\n+        def transform(self, func, *args, **kwargs):\n+            result = self.agg(func, *args, **kwargs)\n+            if is_scalar(result) or len(result) != len(self):\n+                raise ValueError(\"transforms cannot produce \"\n+                                 \"aggregated results\")\n+\n+            return result\n+\n+        cls.transform = transform\n+\n \n def _doc_parms(cls):\n     \"\"\"Return a tuple of the doc parms.\"\"\"\ndiff --git a\/pandas\/core\/series.py b\/pandas\/core\/series.py\n--- a\/pandas\/core\/series.py\n+++ b\/pandas\/core\/series.py\n@@ -2144,6 +2144,49 @@ def map_f(values, f):\n         return self._constructor(new_values,\n                                  index=self.index).__finalize__(self)\n \n+    def _gotitem(self, key, ndim, subset=None):\n+        \"\"\"\n+        sub-classes to define\n+        return a sliced object\n+\n+        Parameters\n+        ----------\n+        key : string \/ list of selections\n+        ndim : 1,2\n+            requested ndim of result\n+        subset : object, default None\n+            subset to act on\n+        \"\"\"\n+        return self\n+\n+    @Appender(generic._shared_docs['aggregate'] % _shared_doc_kwargs)\n+    def aggregate(self, func, axis=0, *args, **kwargs):\n+        axis = self._get_axis_number(axis)\n+        result, how = self._aggregate(func, *args, **kwargs)\n+        if result is None:\n+\n+            # we can be called from an inner function which\n+            # passes this meta-data\n+            kwargs.pop('_axis', None)\n+            kwargs.pop('_level', None)\n+\n+            # try a regular apply, this evaluates lambdas\n+            # row-by-row; however if the lambda is expected a Series\n+            # expression, e.g.: lambda x: x-x.quantile(0.25)\n+            # this will fail, so we can try a vectorized evaluation\n+\n+            # we cannot FIRST try the vectorized evaluation, becuase\n+            # then .agg and .apply would have different semantics if the\n+            # operation is actually defined on the Series, e.g. str\n+            try:\n+                result = self.apply(func, *args, **kwargs)\n+            except (ValueError, AttributeError, TypeError):\n+                result = func(self, *args, **kwargs)\n+\n+        return result\n+\n+    agg = aggregate\n+\n     def apply(self, func, convert_dtype=True, args=(), **kwds):\n         \"\"\"\n         Invoke function on values of Series. Can be ufunc (a NumPy function\n@@ -2167,6 +2210,8 @@ def apply(self, func, convert_dtype=True, args=(), **kwds):\n         See also\n         --------\n         Series.map: For element-wise operations\n+        Series.agg: only perform aggregating type operations\n+        Series.transform: only perform transformating type operations\n \n         Examples\n         --------\n@@ -2244,6 +2289,15 @@ def apply(self, func, convert_dtype=True, args=(), **kwds):\n             return self._constructor(dtype=self.dtype,\n                                      index=self.index).__finalize__(self)\n \n+        # dispatch to agg\n+        if isinstance(func, (list, dict)):\n+            return self.aggregate(func, *args, **kwds)\n+\n+        # if we are a string, try to dispatch\n+        if isinstance(func, compat.string_types):\n+            return self._try_aggregate_string_function(func, *args, **kwds)\n+\n+        # handle ufuncs and lambdas\n         if kwds or args and not isinstance(func, np.ufunc):\n             f = lambda x: func(x, *args, **kwds)\n         else:\n@@ -2253,6 +2307,7 @@ def apply(self, func, convert_dtype=True, args=(), **kwds):\n             if isinstance(f, np.ufunc):\n                 return f(self)\n \n+            # row-wise access\n             if is_extension_type(self.dtype):\n                 mapped = self._values.map(f)\n             else:\n\n<\/patch>",
        "test_patch": ""
      },
      {
        "issue": "BUG: Regression cannot create Series with dtype 'S|' or 'bytes'\n- [X] I have checked that this issue has not already been reported.\r\n\r\n- [X] I have confirmed this bug exists on the latest version of pandas.\r\n\r\n- [ ] (optional) I have confirmed this bug exists on the master branch of pandas.\r\n\r\n---\r\n\r\n#### Code Sample, a copy-pastable example\r\n\r\n```python\r\ns = pd.Series(['foo', 'bar', 'baz'])\r\nassert s.astype(bytes).dtype.kind == 'S', \"kind is not S!\"    # works in 1.1.x, 1.2.0, does not work in 1.2.1 (kind is 'O')\r\n```\r\n\r\n#### Problem description\r\nSetting the type of a series to `bytes` changes str instances to `bytes`, but keeps the type as `object`. This seems to be a regression, in prior versions the type was changed to `bytesXX`. In pandas 1.2.1:\r\n\r\n```py\r\nIn [1]: import pandas as pd                               \r\n                                                                                                                                                                                                                                        \r\nIn [2]: pd.__version__                     \r\nOut[2]: '1.2.1'                               \r\n                                                                                                                    \r\nIn [3]: s = pd.Series(['foo', 'bar', 'baz'])                                                                                                                                                                                            \r\n                                                                                                                    \r\nIn [4]: s.astype('|S')\r\nOut[4]: \r\n0    b'foo'\r\n1    b'bar'\r\n2    b'baz'\r\ndtype: object\r\n\r\nIn [5]: s.astype('bytes')\r\nOut[5]: \r\n0    b'foo'\r\n1    b'bar'\r\n2    b'baz'\r\ndtype: object\r\n\r\nIn [6]: s.astype(bytes)\r\nOut[6]: \r\n0    b'foo'\r\n1    b'bar'\r\n2    b'baz'\r\ndtype: object\r\n```\r\n\r\n#### Expected Output\r\nI expected this output, produced with 1.2.0:\r\n\r\n```py\r\nIn [1]: import pandas as pd\r\n\r\nIn [2]: pd.__version__\r\nOut[2]: '1.2.0'\r\n\r\nIn [3]: s = pd.Series(['foo', 'bar', 'baz'])\r\n\r\nIn [4]: s.astype('|S')\r\nOut[4]: \r\n0    b'foo'\r\n1    b'bar'\r\n2    b'baz'\r\ndtype: bytes24\r\n\r\nIn [5]: s.astype('bytes')\r\nOut[5]: \r\n0    b'foo'\r\n1    b'bar'\r\n2    b'baz'\r\ndtype: bytes24\r\n\r\nIn [6]: s.astype(bytes)\r\nOut[6]: \r\n0    b'foo'\r\n1    b'bar'\r\n2    b'baz'\r\ndtype: bytes24\r\n```\r\n\r\n#### Output of ``pd.show_versions()``\r\n\r\n<details>\r\n\r\nINSTALLED VERSIONS     \r\n------------------     \r\ncommit           : 9d598a5e1eee26df95b3910e3f2934890d062caa\r\npython           : 3.8.6.final.0\r\npython-bits      : 64  \r\nOS               : Linux\r\nOS-release       : 5.7.17-1rodete4-amd64\r\nVersion          : #1 SMP Debian 5.7.17-1rodete4 (2020-10-01)\r\nmachine          : x86_64\r\nprocessor        : \r\nbyteorder        : little\r\nLC_ALL           : None\r\nLANG             : en_US.UTF-8\r\nLOCALE           : en_US.UTF-8\r\n\r\npandas           : 1.2.1\r\nnumpy            : 1.19.4\r\npytz             : 2020.4\r\ndateutil         : 2.8.1\r\npip              : 20.2.1\r\nsetuptools       : 49.2.1\r\nCython           : 0.29.21\r\npytest           : 4.6.11\r\nhypothesis       : None\r\nsphinx           : 1.8.5\r\nblosc            : None\r\nfeather          : None\r\nxlsxwriter       : None\r\nlxml.etree       : None\r\nhtml5lib         : None\r\npymysql          : None\r\npsycopg2         : 2.8.6 (dt dec pq3 ext lo64)\r\njinja2           : 2.11.2\r\nIPython          : 7.19.0\r\npandas_datareader: None\r\nbs4              : None\r\nbottleneck       : None\r\nfsspec           : None\r\nfastparquet      : None\r\ngcsfs            : None\r\nmatplotlib       : None\r\nnumexpr          : None\r\nodfpy            : None\r\nopenpyxl         : None\r\npandas_gbq       : None\r\npyarrow          : 0.17.1\r\npyxlsb           : None\r\ns3fs             : None\r\nscipy            : None\r\nsqlalchemy       : 1.3.20\r\ntables           : None\r\ntabulate         : None\r\nxarray           : None\r\nxlrd             : None\r\nxlwt             : None\r\nnumba            : None\r\n\r\n<\/details>\r\n\n",
        "patch": "<patch>\ndiff --git a\/doc\/source\/whatsnew\/v1.2.2.rst b\/doc\/source\/whatsnew\/v1.2.2.rst\n--- a\/doc\/source\/whatsnew\/v1.2.2.rst\n+++ b\/doc\/source\/whatsnew\/v1.2.2.rst\n@@ -17,6 +17,7 @@ Fixed regressions\n \n - Fixed regression in :func:`read_excel` that caused it to raise ``AttributeError`` when checking version of older xlrd versions (:issue:`38955`)\n - Fixed regression in :class:`DataFrame` constructor reordering element when construction from datetime ndarray with dtype not ``\"datetime64[ns]\"`` (:issue:`39422`)\n+- Fixed regression in :class:`DataFrame.astype` and :class:`Series.astype` not casting to bytes dtype (:issue:`39474`)\n - Fixed regression in :meth:`~DataFrame.to_pickle` failing to create bz2\/xz compressed pickle files with ``protocol=5`` (:issue:`39002`)\n - Fixed regression in :func:`pandas.testing.assert_series_equal` and :func:`pandas.testing.assert_frame_equal` always raising ``AssertionError`` when comparing extension dtypes (:issue:`39410`)\n - Fixed regression in :meth:`~DataFrame.to_csv` opening ``codecs.StreamWriter`` in binary mode instead of in text mode and ignoring user-provided ``mode`` (:issue:`39247`)\ndiff --git a\/pandas\/core\/dtypes\/cast.py b\/pandas\/core\/dtypes\/cast.py\n--- a\/pandas\/core\/dtypes\/cast.py\n+++ b\/pandas\/core\/dtypes\/cast.py\n@@ -1221,7 +1221,7 @@ def soft_convert_objects(\n             values = lib.maybe_convert_objects(\n                 values, convert_datetime=datetime, convert_timedelta=timedelta\n             )\n-        except OutOfBoundsDatetime:\n+        except (OutOfBoundsDatetime, ValueError):\n             return values\n \n     if numeric and is_object_dtype(values.dtype):\ndiff --git a\/pandas\/core\/internals\/blocks.py b\/pandas\/core\/internals\/blocks.py\n--- a\/pandas\/core\/internals\/blocks.py\n+++ b\/pandas\/core\/internals\/blocks.py\n@@ -2254,7 +2254,7 @@ class ObjectBlock(Block):\n     _can_hold_na = True\n \n     def _maybe_coerce_values(self, values):\n-        if issubclass(values.dtype.type, (str, bytes)):\n+        if issubclass(values.dtype.type, str):\n             values = np.array(values, dtype=object)\n         return values\n\n<\/patch>",
        "test_patch": ""
      }
    ],
    "signature_instructions": "Given the fields `issue`, produce the fields `patch`.",
    "signature_prefix": "Patch:",
    "extended_signature_instructions": "Given the fields `issue`, produce the fields `patch`.",
    "extended_signature_prefix": "Patch:"
  }
}