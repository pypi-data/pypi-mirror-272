{
  "prog": {
    "lm": null,
    "traces": [],
    "train": [],
    "demos": [
      {
        "issue": "[BUG] Groupby with as_index=False raises error when type is Category.\n#### Code to reproduce the issue\r\n\r\n```python\r\nimport pandas as pd\r\n\r\ntest = pd.DataFrame([[1, 1], [2, 2], [3, 3]], columns=['col1', 'col2'])\r\ntest['col1'] = test['col1'].astype('category')\r\n\r\ntest.groupby(['col1', 'col2'], as_index=False).size()\r\n```\r\n#### Problem description\r\n\r\nWith pandas 1.0.1, the code throws an error `ValueError: No axis named 1 for object type <class 'pandas.core.series.Series'>`.\r\nWith pandas 0.25.3, the code works, but `as_index` argument do not function as already mentioned in #25011.\r\nThis happened with categorical type, the output of the new version is similar to 0.25.3 with other types.\r\n\r\n#### Expected Output\r\n|    |   col1 |   col2 |   0 |\r\n|---:|-------:|-------:|----:|\r\n|  0 |      1 |      1 |   1 |\r\n|  1 |      1 |      2 |   0 |\r\n|  2 |      1 |      3 |   0 |\r\n|  3 |      2 |      1 |   0 |\r\n|  4 |      2 |      2 |   1 |\r\n|  5 |      2 |      3 |   0 |\r\n|  6 |      3 |      1 |   0 |\r\n|  7 |      3 |      2 |   0 |\r\n|  8 |      3 |      3 |   1 |\r\n\r\n#### Output of ``pd.show_versions()``\r\n\r\n<details>\r\n\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit           : None\r\npython           : 3.8.1.final.0\r\npython-bits      : 64\r\nOS               : Darwin\r\nOS-release       : 19.3.0\r\nmachine          : x86_64\r\nprocessor        : i386\r\nbyteorder        : little\r\nLC_ALL           : None\r\nLANG             : None\r\nLOCALE           : en_US.UTF-8\r\n\r\npandas           : 1.0.1\r\nnumpy            : 1.18.1\r\npytz             : 2019.3\r\ndateutil         : 2.8.1\r\npip              : 20.0.2\r\nsetuptools       : 45.2.0.post20200210\r\nCython           : None\r\npytest           : 5.3.2\r\nhypothesis       : None\r\nsphinx           : None\r\nblosc            : None\r\nfeather          : None\r\nxlsxwriter       : None\r\nlxml.etree       : None\r\nhtml5lib         : 1.0.1\r\npymysql          : None\r\npsycopg2         : None\r\njinja2           : 2.10.3\r\nIPython          : None\r\npandas_datareader: None\r\nbs4              : 4.8.2\r\nbottleneck       : None\r\nfastparquet      : None\r\ngcsfs            : None\r\nlxml.etree       : None\r\nmatplotlib       : 3.1.3\r\nnumexpr          : None\r\nodfpy            : None\r\nopenpyxl         : None\r\npandas_gbq       : None\r\npyarrow          : None\r\npytables         : None\r\npytest           : 5.3.2\r\npyxlsb           : None\r\ns3fs             : None\r\nscipy            : 1.4.1\r\nsqlalchemy       : None\r\ntables           : None\r\ntabulate         : 0.8.6\r\nxarray           : None\r\nxlrd             : None\r\nxlwt             : None\r\nxlsxwriter       : None\r\nnumba            : None\r\n\r\n<\/details>\r\n\n\n\n> With pandas 1.0.1, the code throws an error\r\n\r\nregression in #29690 (i.e. 1.0.0)\r\n\r\nc5a1f9e2c373ced9ef2f02ab64d11eaa7b4248f2 is the first bad commit\r\ncommit c5a1f9e2c373ced9ef2f02ab64d11eaa7b4248f2\r\nAuthor: Oliver Hofkens <oliver@novemberfive.co>\r\nDate:   Wed Nov 20 13:46:18 2019 +0100\r\n\r\n    BUG: Series groupby does not include nan counts for all categorical labels (#17605) (#29690)\r\n\r\n\r\n",
        "patch": "<patch>\ndiff --git a\/doc\/source\/whatsnew\/v1.1.0.rst b\/doc\/source\/whatsnew\/v1.1.0.rst\n--- a\/doc\/source\/whatsnew\/v1.1.0.rst\n+++ b\/doc\/source\/whatsnew\/v1.1.0.rst\n@@ -670,6 +670,25 @@ Using :meth:`DataFrame.groupby` with ``as_index=False`` and the function ``idxma\n \n    df.groupby(\"a\", as_index=False).nunique()\n \n+The method :meth:`core.DataFrameGroupBy.size` would previously ignore ``as_index=False``. Now the grouping columns are returned as columns, making the result a `DataFrame` instead of a `Series`. (:issue:`32599`)\n+\n+*Previous behavior*:\n+\n+.. code-block:: ipython\n+\n+   In [3]: df.groupby(\"a\", as_index=False).size()\n+   Out[4]:\n+   a\n+   x    2\n+   y    2\n+   dtype: int64\n+\n+*New behavior*:\n+\n+.. ipython:: python\n+\n+   df.groupby(\"a\", as_index=False).size()\n+\n .. _whatsnew_110.api_breaking.apply_applymap_first_once:\n \n apply and applymap on ``DataFrame`` evaluates first row\/column only once\n@@ -983,6 +1002,7 @@ Groupby\/resample\/rolling\n   The behaviour now is consistent, independent of internal heuristics. (:issue:`31612`, :issue:`14927`, :issue:`13056`)\n - Bug in :meth:`SeriesGroupBy.agg` where any column name was accepted in the named aggregation of ``SeriesGroupBy`` previously. The behaviour now allows only ``str`` and callables else would raise ``TypeError``. (:issue:`34422`)\n \n+\n Reshaping\n ^^^^^^^^^\n \ndiff --git a\/pandas\/core\/frame.py b\/pandas\/core\/frame.py\n--- a\/pandas\/core\/frame.py\n+++ b\/pandas\/core\/frame.py\n@@ -5440,7 +5440,7 @@ def value_counts(\n         if subset is None:\n             subset = self.columns.tolist()\n \n-        counts = self.groupby(subset).size()\n+        counts = self.groupby(subset).grouper.size()\n \n         if sort:\n             counts = counts.sort_values(ascending=ascending)\ndiff --git a\/pandas\/core\/groupby\/groupby.py b\/pandas\/core\/groupby\/groupby.py\n--- a\/pandas\/core\/groupby\/groupby.py\n+++ b\/pandas\/core\/groupby\/groupby.py\n@@ -942,9 +942,9 @@ def _transform_should_cast(self, func_nm: str) -> bool:\n         bool\n             Whether transform should attempt to cast the result of aggregation\n         \"\"\"\n-        return (self.size().fillna(0) > 0).any() and (\n-            func_nm not in base.cython_cast_blacklist\n-        )\n+        filled_series = self.grouper.size().fillna(0)\n+        assert filled_series is not None\n+        return filled_series.gt(0).any() and func_nm not in base.cython_cast_blacklist\n \n     def _cython_transform(self, how: str, numeric_only: bool = True, **kwargs):\n         output: Dict[base.OutputKey, np.ndarray] = {}\n@@ -1507,14 +1507,15 @@ def sem(self, ddof: int = 1):\n \n     @Substitution(name=\"groupby\")\n     @Appender(_common_see_also)\n-    def size(self):\n+    def size(self) -> FrameOrSeriesUnion:\n         \"\"\"\n         Compute group sizes.\n \n         Returns\n         -------\n-        Series\n-            Number of rows in each group.\n+        DataFrame or Series\n+            Number of rows in each group as a Series if as_index is True\n+            or a DataFrame if as_index is False.\n         \"\"\"\n         result = self.grouper.size()\n \n@@ -1523,6 +1524,10 @@ def size(self):\n             result = self._obj_1d_constructor(result, name=self.obj.name)\n         else:\n             result = self._obj_1d_constructor(result)\n+\n+        if not self.as_index:\n+            result = result.rename(\"size\").reset_index()\n+\n         return self._reindex_output(result, fill_value=0)\n \n     @doc(_groupby_agg_method_template, fname=\"sum\", no=True, mc=0)\n\n<\/patch>",
        "test_patch": ""
      },
      {
        "issue": "Feature request: add median, mode & number of unique entries to pandas.DataFrame.describe()\nWould come in handy for a number of a different applications from basic statistics, to understanding one's data, to estimating machine learning algorithmic load.\n\ndupe of #2749\n\n\n\nFYI, you can easily make your own and patch it in. Just put in your startup code \/ application.\n\n```\ndef describe(self):\n    \"\"\" describe  of a series \"\"\"\n    l = [ ('nobs'  , len(self.index)),\n          ('valid' , self.count()   ),\n          ('mean'  , self.mean()    ),\n          ('min'   , self.min()     ),\n          ('max'   , self.max()     ),\n          ('std'   , self.std()     ),\n          ('10%'   , self.quantile(0.10)),\n          ('25%'   , self.quantile(0.25)),\n          ('50%'   , self.median()  ),\n          ('75%'   , self.quantile(0.75)),\n          ('90%'   , self.quantile(0.90)),\n          ('skew'  , self.skew()    ),\n          ('kurt'  , self.kurt()    ) ]\n    s = Series(dict(l), index = [ k for k, v in l ])\n    s[s.abs()<0.000001] = 0.0\n    return s\nSeries.describe = describe\n```\n\n`median` out-of-the-box would really be a great thing to have!\nNot just for `Series` but also for `GroupBy`'s.\n\nNote that median is already in describe, as in `50%`\n\n@jorisvandenbossche indeed! Sorry, my bad. :baby: \n\nHijacking this issue to propose a variant (and steal more from dplyr)\n\nWhy not have a `DataFrame.agg` that is identical to `DataFrame.groupby.agg`, but with a \"single group\"? This also goes along with the new `.resample.agg`, yay for synergy. Basically it takes a function\/str or list of functions\/strs or a dict of column names to functions\/str and aggregates accordingly.\n\nRelated to https:\/\/github.com\/pydata\/pandas\/issues\/1623\n",
        "patch": "<patch>\ndiff --git a\/doc\/source\/api.rst b\/doc\/source\/api.rst\n--- a\/doc\/source\/api.rst\n+++ b\/doc\/source\/api.rst\n@@ -314,6 +314,8 @@ Function application, GroupBy & Window\n    :toctree: generated\/\n \n    Series.apply\n+   Series.aggregate\n+   Series.transform\n    Series.map\n    Series.groupby\n    Series.rolling\n@@ -831,6 +833,8 @@ Function application, GroupBy & Window\n \n    DataFrame.apply\n    DataFrame.applymap\n+   DataFrame.aggregate\n+   DataFrame.transform\n    DataFrame.groupby\n    DataFrame.rolling\n    DataFrame.expanding\ndiff --git a\/doc\/source\/basics.rst b\/doc\/source\/basics.rst\n--- a\/doc\/source\/basics.rst\n+++ b\/doc\/source\/basics.rst\n@@ -702,7 +702,8 @@ on an entire ``DataFrame`` or ``Series``, row- or column-wise, or elementwise.\n \n 1. `Tablewise Function Application`_: :meth:`~DataFrame.pipe`\n 2. `Row or Column-wise Function Application`_: :meth:`~DataFrame.apply`\n-3. Elementwise_ function application: :meth:`~DataFrame.applymap`\n+3. `Aggregation API`_: :meth:`~DataFrame.agg` and :meth:`~DataFrame.transform`\n+4. `Applying Elementwise Functions`_: :meth:`~DataFrame.applymap`\n \n .. _basics.pipe:\n \n@@ -778,6 +779,13 @@ statistics methods, take an optional ``axis`` argument:\n    df.apply(np.cumsum)\n    df.apply(np.exp)\n \n+``.apply()`` will also dispatch on a string method name.\n+\n+.. ipython:: python\n+\n+   df.apply('mean')\n+   df.apply('mean', axis=1)\n+\n Depending on the return type of the function passed to :meth:`~DataFrame.apply`,\n the result will either be of lower dimension or the same dimension.\n \n@@ -827,16 +835,223 @@ set to True, the passed function will instead receive an ndarray object, which\n has positive performance implications if you do not need the indexing\n functionality.\n \n-.. seealso::\n+.. _basics.aggregate:\n+\n+Aggregation API\n+~~~~~~~~~~~~~~~\n+\n+.. versionadded:: 0.20.0\n+\n+The aggregation API allows one to express possibly multiple aggregation operations in a single concise way.\n+This API is similar across pandas objects, see :ref:`groupby API <groupby.aggregate>`, the\n+:ref:`window functions API <stats.aggregate>`, and the :ref:`resample API <timeseries.aggregate>`.\n+The entry point for aggregation is the method :meth:`~DataFrame.aggregate`, or the alias :meth:`~DataFrame.agg`.\n+\n+We will use a similar starting frame from above:\n+\n+.. ipython:: python\n+\n+   tsdf = pd.DataFrame(np.random.randn(10, 3), columns=['A', 'B', 'C'],\n+                       index=pd.date_range('1\/1\/2000', periods=10))\n+   tsdf.iloc[3:7] = np.nan\n+   tsdf\n+\n+Using a single function is equivalent to :meth:`~DataFrame.apply`; You can also pass named methods as strings.\n+These will return a ``Series`` of the aggregated output:\n+\n+.. ipython:: python\n+\n+   tsdf.agg(np.sum)\n+\n+   tsdf.agg('sum')\n+\n+   # these are equivalent to a ``.sum()`` because we are aggregating on a single function\n+   tsdf.sum()\n+\n+Single aggregations on a ``Series`` this will result in a scalar value:\n+\n+.. ipython:: python\n+\n+   tsdf.A.agg('sum')\n+\n+\n+Aggregating with multiple functions\n++++++++++++++++++++++++++++++++++++\n+\n+You can pass multiple aggregation arguments as a list.\n+The results of each of the passed functions will be a row in the resultant ``DataFrame``.\n+These are naturally named from the aggregation function.\n+\n+.. ipython:: python\n+\n+   tsdf.agg(['sum'])\n+\n+Multiple functions yield multiple rows:\n+\n+.. ipython:: python\n+\n+   tsdf.agg(['sum', 'mean'])\n+\n+On a ``Series``, multiple functions return a ``Series``, indexed by the function names:\n+\n+.. ipython:: python\n+\n+   tsdf.A.agg(['sum', 'mean'])\n+\n+Passing a ``lambda`` function will yield a ``<lambda>`` named row:\n+\n+.. ipython:: python\n+\n+   tsdf.A.agg(['sum', lambda x: x.mean()])\n+\n+Passing a named function will yield that name for the row:\n+\n+.. ipython:: python\n+\n+   def mymean(x):\n+      return x.mean()\n+\n+   tsdf.A.agg(['sum', mymean])\n+\n+Aggregating with a dict\n++++++++++++++++++++++++\n+\n+Passing a dictionary of column names to a scalar or a list of scalars, to ``DataFame.agg``\n+allows you to customize which functions are applied to which columns.\n+\n+.. ipython:: python\n+\n+   tsdf.agg({'A': 'mean', 'B': 'sum'})\n+\n+Passing a list-like will generate a ``DataFrame`` output. You will get a matrix-like output\n+of all of the aggregators. The output will consist of all unique functions. Those that are\n+not noted for a particular column will be ``NaN``:\n+\n+.. ipython:: python\n+\n+   tsdf.agg({'A': ['mean', 'min'], 'B': 'sum'})\n+\n+.. _basics.aggregation.mixed_dtypes:\n+\n+Mixed Dtypes\n+++++++++++++\n \n-   The section on :ref:`GroupBy <groupby>` demonstrates related, flexible\n-   functionality for grouping by some criterion, applying, and combining the\n-   results into a Series, DataFrame, etc.\n+When presented with mixed dtypes that cannot aggregate, ``.agg`` will only take the valid\n+aggregations. This is similiar to how groupby ``.agg`` works.\n \n-.. _Elementwise:\n+.. ipython:: python\n+\n+   mdf = pd.DataFrame({'A': [1, 2, 3],\n+                       'B': [1., 2., 3.],\n+                       'C': ['foo', 'bar', 'baz'],\n+                       'D': pd.date_range('20130101', periods=3)})\n+   mdf.dtypes\n+\n+.. ipython:: python\n+\n+   mdf.agg(['min', 'sum'])\n+\n+.. _basics.aggregation.custom_describe:\n+\n+Custom describe\n++++++++++++++++\n+\n+With ``.agg()`` is it possible to easily create a custom describe function, similar\n+to the built in :ref:`describe function <basics.describe>`.\n+\n+.. ipython:: python\n+\n+   from functools import partial\n+\n+   q_25 = partial(pd.Series.quantile, q=0.25)\n+   q_25.__name__ = '25%'\n+   q_75 = partial(pd.Series.quantile, q=0.75)\n+   q_75.__name__ = '75%'\n+\n+   tsdf.agg(['count', 'mean', 'std', 'min', q_25, 'median', q_75, 'max'])\n+\n+.. _basics.transform:\n+\n+Transform API\n+~~~~~~~~~~~~~\n \n-Applying elementwise Python functions\n-~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n+.. versionadded:: 0.20.0\n+\n+The :method:`~DataFrame.transform` method returns an object that is indexed the same (same size)\n+as the original. This API allows you to provide *multiple* operations at the same\n+time rather than one-by-one. Its api is quite similar to the ``.agg`` API.\n+\n+Use a similar frame to the above sections.\n+\n+.. ipython:: python\n+\n+   tsdf = pd.DataFrame(np.random.randn(10, 3), columns=['A', 'B', 'C'],\n+                       index=pd.date_range('1\/1\/2000', periods=10))\n+   tsdf.iloc[3:7] = np.nan\n+   tsdf\n+\n+Transform the entire frame. Transform allows functions to input as a numpy function, string\n+function name and user defined function.\n+\n+.. ipython:: python\n+\n+   tsdf.transform(np.abs)\n+   tsdf.transform('abs')\n+   tsdf.transform(lambda x: x.abs())\n+\n+Since this is a single function, this is equivalent to a ufunc application\n+\n+.. ipython:: python\n+\n+   np.abs(tsdf)\n+\n+Passing a single function to ``.transform()`` with a Series will yield a single Series in return.\n+\n+.. ipython:: python\n+\n+   tsdf.A.transform(np.abs)\n+\n+\n+Transform with multiple functions\n++++++++++++++++++++++++++++++++++\n+\n+Passing multiple functions will yield a column multi-indexed DataFrame.\n+The first level will be the original frame column names; the second level\n+will be the names of the transforming functions.\n+\n+.. ipython:: python\n+\n+   tsdf.transform([np.abs, lambda x: x+1])\n+\n+Passing multiple functions to a Series will yield a DataFrame. The\n+resulting column names will be the transforming functions.\n+\n+.. ipython:: python\n+\n+   tsdf.A.transform([np.abs, lambda x: x+1])\n+\n+\n+Transforming with a dict\n+++++++++++++++++++++++++\n+\n+\n+Passing a dict of functions will will allow selective transforming per column.\n+\n+.. ipython:: python\n+\n+   tsdf.transform({'A': np.abs, 'B': lambda x: x+1})\n+\n+Passing a dict of lists will generate a multi-indexed DataFrame with these\n+selective transforms.\n+\n+.. ipython:: python\n+\n+   tsdf.transform({'A': np.abs, 'B': [lambda x: x+1, 'sqrt']})\n+\n+.. _basics.elementwise:\n+\n+Applying Elementwise Functions\n+~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n \n Since not all functions can be vectorized (accept NumPy arrays and return\n another array or value), the methods :meth:`~DataFrame.applymap` on DataFrame\ndiff --git a\/doc\/source\/computation.rst b\/doc\/source\/computation.rst\n--- a\/doc\/source\/computation.rst\n+++ b\/doc\/source\/computation.rst\n@@ -617,7 +617,9 @@ Aggregation\n -----------\n \n Once the ``Rolling``, ``Expanding`` or ``EWM`` objects have been created, several methods are available to\n-perform multiple computations on the data. This is very similar to a ``.groupby(...).agg`` seen :ref:`here <groupby.aggregate>`.\n+perform multiple computations on the data. These operations are similar to the :ref:`aggregating API <basics.aggregate>`,\n+:ref:`groupby aggregates <groupby.aggregate>`, and :ref:`resample API <timeseries.aggregate>`.\n+\n \n .. ipython:: python\n \n@@ -642,10 +644,10 @@ columns if none are selected.\n \n .. _stats.aggregate.multifunc:\n \n-Applying multiple functions at once\n-~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n+Applying multiple functions\n+~~~~~~~~~~~~~~~~~~~~~~~~~~~\n \n-With windowed Series you can also pass a list or dict of functions to do\n+With windowed ``Series`` you can also pass a list of functions to do\n aggregation with, outputting a DataFrame:\n \n .. ipython:: python\n@@ -666,7 +668,7 @@ Applying different functions to DataFrame columns\n ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n \n By passing a dict to ``aggregate`` you can apply a different aggregation to the\n-columns of a DataFrame:\n+columns of a ``DataFrame``:\n \n .. ipython:: python\n    :okexcept:\ndiff --git a\/doc\/source\/groupby.rst b\/doc\/source\/groupby.rst\n--- a\/doc\/source\/groupby.rst\n+++ b\/doc\/source\/groupby.rst\n@@ -439,7 +439,9 @@ Aggregation\n -----------\n \n Once the GroupBy object has been created, several methods are available to\n-perform a computation on the grouped data.\n+perform a computation on the grouped data. These operations are similar to the\n+:ref:`aggregating API <basics.aggregate>`, :ref:`window functions API <stats.aggregate>`,\n+and :ref:`resample API <timeseries.aggregate>`.\n \n An obvious one is aggregation via the ``aggregate`` or equivalently ``agg`` method:\n \ndiff --git a\/doc\/source\/timeseries.rst b\/doc\/source\/timeseries.rst\n--- a\/doc\/source\/timeseries.rst\n+++ b\/doc\/source\/timeseries.rst\n@@ -1519,11 +1519,13 @@ We can instead only resample those groups where we have points as follows:\n \n     ts.groupby(partial(round, freq='3T')).sum()\n \n+.. _timeseries.aggregate:\n+\n Aggregation\n ~~~~~~~~~~~\n \n-Similar to :ref:`groupby aggregates <groupby.aggregate>` and the :ref:`window functions <stats.aggregate>`, a ``Resampler`` can be selectively\n-resampled.\n+Similar to the :ref:`aggregating API <basics.aggregate>`, :ref:`groupby aggregates API <groupby.aggregate>`, and  the :ref:`window functions API <stats.aggregate>`,\n+a ``Resampler`` can be selectively resampled.\n \n Resampling a ``DataFrame``, the default will be to act on all columns with the same function.\n \ndiff --git a\/doc\/source\/whatsnew\/v0.20.0.txt b\/doc\/source\/whatsnew\/v0.20.0.txt\n--- a\/doc\/source\/whatsnew\/v0.20.0.txt\n+++ b\/doc\/source\/whatsnew\/v0.20.0.txt\n@@ -9,6 +9,8 @@ users upgrade to this version.\n \n Highlights include:\n \n+- new ``.agg()`` API for Series\/DataFrame similar to the groupby-rolling-resample API's, see :ref:`here <whatsnew_0200.enhancements.agg>`\n+- Integration with the ``feather-format``, including a new top-level ``pd.read_feather()`` and ``DataFrame.to_feather()`` method, see :ref:`here <io.feather>`.\n - The ``.ix`` indexer has been deprecated, see :ref:`here <whatsnew_0200.api_breaking.deprecate_ix>`\n - ``Panel`` has been deprecated, see :ref:`here <whatsnew_0200.api_breaking.deprecate_panel>`\n - Improved user API when accessing levels in ``.groupby()``, see :ref:`here <whatsnew_0200.enhancements.groupby_access>`\n@@ -32,6 +34,68 @@ Check the :ref:`API Changes <whatsnew_0200.api_breaking>` and :ref:`deprecations\n New features\n ~~~~~~~~~~~~\n \n+.. _whatsnew_0200.enhancements.agg:\n+\n+``agg`` API\n+^^^^^^^^^^^\n+\n+Series & DataFrame have been enhanced to support the aggregation API. This is an already familiar API that\n+is supported for groupby, window operations, and resampling. This allows one to express, possibly multiple\n+aggregation operations, in a single concise way by using :meth:`~DataFrame.agg`,\n+and :meth:`~DataFrame.transform`. The full documentation is :ref:`here <basics.aggregate>`` (:issue:`1623`)\n+\n+Here is a sample\n+\n+.. ipython:: python\n+\n+   df = pd.DataFrame(np.random.randn(10, 3), columns=['A', 'B', 'C'],\n+                    index=pd.date_range('1\/1\/2000', periods=10))\n+   df.iloc[3:7] = np.nan\n+   df\n+\n+One can operate using string function names, callables, lists, or dictionaries of these.\n+\n+Using a single function is equivalent to ``.apply``.\n+\n+.. ipython:: python\n+\n+   df.agg('sum')\n+\n+Multiple functions in lists.\n+\n+.. ipython:: python\n+\n+   df.agg(['sum', 'min'])\n+\n+Dictionaries to provide the ability to provide selective aggregation per column.\n+You will get a matrix-like output of all of the aggregators. The output will consist\n+of all unique functions. Those that are not noted for a particular column will be ``NaN``:\n+\n+.. ipython:: python\n+\n+   df.agg({'A' : ['sum', 'min'], 'B' : ['min', 'max']})\n+\n+The API also supports a ``.transform()`` function to provide for broadcasting results.\n+\n+.. ipython:: python\n+\n+   df.transform(['abs', lambda x: x - x.min()])\n+\n+When presented with mixed dtypes that cannot aggregate, ``.agg()`` will only take the valid\n+aggregations. This is similiar to how groupby ``.agg()`` works. (:issue:`15015`)\n+\n+.. ipython:: python\n+\n+   df = pd.DataFrame({'A': [1, 2, 3],\n+                      'B': [1., 2., 3.],\n+                      'C': ['foo', 'bar', 'baz'],\n+                      'D': pd.date_range('20130101', periods=3)})\n+   df.dtypes\n+\n+.. ipython:: python\n+\n+   df.agg(['min', 'sum'])\n+\n .. _whatsnew_0200.enhancements.dataio_dtype:\n \n ``dtype`` keyword for data IO\ndiff --git a\/pandas\/core\/base.py b\/pandas\/core\/base.py\n--- a\/pandas\/core\/base.py\n+++ b\/pandas\/core\/base.py\n@@ -470,6 +470,15 @@ def _aggregate(self, arg, *args, **kwargs):\n \n             obj = self._selected_obj\n \n+            def nested_renaming_depr(level=4):\n+                # deprecation of nested renaming\n+                # GH 15931\n+                warnings.warn(\n+                    (\"using a dict with renaming \"\n+                     \"is deprecated and will be removed in a future \"\n+                     \"version\"),\n+                    FutureWarning, stacklevel=level)\n+\n             # if we have a dict of any non-scalars\n             # eg. {'A' : ['mean']}, normalize all to\n             # be list-likes\n@@ -498,14 +507,10 @@ def _aggregate(self, arg, *args, **kwargs):\n                             raise SpecificationError('cannot perform renaming '\n                                                      'for {0} with a nested '\n                                                      'dictionary'.format(k))\n+                        nested_renaming_depr(4 + (_level or 0))\n \n-                        # deprecation of nested renaming\n-                        # GH 15931\n-                        warnings.warn(\n-                            (\"using a dict with renaming \"\n-                             \"is deprecated and will be removed in a future \"\n-                             \"version\"),\n-                            FutureWarning, stacklevel=4)\n+                    elif isinstance(obj, ABCSeries):\n+                        nested_renaming_depr()\n \n                 arg = new_arg\n \n@@ -515,11 +520,7 @@ def _aggregate(self, arg, *args, **kwargs):\n                 keys = list(compat.iterkeys(arg))\n                 if (isinstance(obj, ABCDataFrame) and\n                         len(obj.columns.intersection(keys)) != len(keys)):\n-                    warnings.warn(\n-                        (\"using a dict with renaming \"\n-                         \"is deprecated and will be removed in a future \"\n-                         \"version\"),\n-                        FutureWarning, stacklevel=4)\n+                    nested_renaming_depr()\n \n             from pandas.tools.concat import concat\n \ndiff --git a\/pandas\/core\/frame.py b\/pandas\/core\/frame.py\n--- a\/pandas\/core\/frame.py\n+++ b\/pandas\/core\/frame.py\n@@ -4189,6 +4189,42 @@ def diff(self, periods=1, axis=0):\n     # ----------------------------------------------------------------------\n     # Function application\n \n+    def _gotitem(self, key, ndim, subset=None):\n+        \"\"\"\n+        sub-classes to define\n+        return a sliced object\n+\n+        Parameters\n+        ----------\n+        key : string \/ list of selections\n+        ndim : 1,2\n+            requested ndim of result\n+        subset : object, default None\n+            subset to act on\n+        \"\"\"\n+        if subset is None:\n+            subset = self\n+\n+        # TODO: _shallow_copy(subset)?\n+        return self[key]\n+\n+    @Appender(_shared_docs['aggregate'] % _shared_doc_kwargs)\n+    def aggregate(self, func, axis=0, *args, **kwargs):\n+        axis = self._get_axis_number(axis)\n+\n+        # TODO: flipped axis\n+        result = None\n+        if axis == 0:\n+            try:\n+                result, how = self._aggregate(func, axis=0, *args, **kwargs)\n+            except TypeError:\n+                pass\n+        if result is None:\n+            return self.apply(func, axis=axis, args=args, **kwargs)\n+        return result\n+\n+    agg = aggregate\n+\n     def apply(self, func, axis=0, broadcast=False, raw=False, reduce=None,\n               args=(), **kwds):\n         \"\"\"\n@@ -4244,22 +4280,35 @@ def apply(self, func, axis=0, broadcast=False, raw=False, reduce=None,\n         See also\n         --------\n         DataFrame.applymap: For elementwise operations\n+        DataFrame.agg: only perform aggregating type operations\n+        DataFrame.transform: only perform transformating type operations\n \n         Returns\n         -------\n         applied : Series or DataFrame\n         \"\"\"\n         axis = self._get_axis_number(axis)\n-        if kwds or args and not isinstance(func, np.ufunc):\n+        ignore_failures = kwds.pop('ignore_failures', False)\n+\n+        # dispatch to agg\n+        if axis == 0 and isinstance(func, (list, dict)):\n+            return self.aggregate(func, axis=axis, *args, **kwds)\n+\n+        if len(self.columns) == 0 and len(self.index) == 0:\n+            return self._apply_empty_result(func, axis, reduce, *args, **kwds)\n \n+        # if we are a string, try to dispatch\n+        if isinstance(func, compat.string_types):\n+            if axis:\n+                kwds['axis'] = axis\n+            return getattr(self, func)(*args, **kwds)\n+\n+        if kwds or args and not isinstance(func, np.ufunc):\n             def f(x):\n                 return func(x, *args, **kwds)\n         else:\n             f = func\n \n-        if len(self.columns) == 0 and len(self.index) == 0:\n-            return self._apply_empty_result(func, axis, reduce, *args, **kwds)\n-\n         if isinstance(f, np.ufunc):\n             with np.errstate(all='ignore'):\n                 results = f(self.values)\n@@ -4276,7 +4325,10 @@ def f(x):\n                 else:\n                     if reduce is None:\n                         reduce = True\n-                    return self._apply_standard(f, axis, reduce=reduce)\n+                    return self._apply_standard(\n+                        f, axis,\n+                        reduce=reduce,\n+                        ignore_failures=ignore_failures)\n             else:\n                 return self._apply_broadcast(f, axis)\n \n@@ -5085,7 +5137,13 @@ def f(x):\n                         # this can end up with a non-reduction\n                         # but not always. if the types are mixed\n                         # with datelike then need to make sure a series\n-                        result = self.apply(f, reduce=False)\n+\n+                        # we only end up here if we have not specified\n+                        # numeric_only and yet we have tried a\n+                        # column-by-column reduction, where we have mixed type.\n+                        # So let's just do what we can\n+                        result = self.apply(f, reduce=False,\n+                                            ignore_failures=True)\n                         if result.ndim == self.ndim:\n                             result = result.iloc[0]\n                         return result\ndiff --git a\/pandas\/core\/generic.py b\/pandas\/core\/generic.py\n--- a\/pandas\/core\/generic.py\n+++ b\/pandas\/core\/generic.py\n@@ -32,7 +32,7 @@\n                                 SettingWithCopyError, SettingWithCopyWarning,\n                                 AbstractMethodError)\n \n-from pandas.core.base import PandasObject\n+from pandas.core.base import PandasObject, SelectionMixin\n from pandas.core.index import (Index, MultiIndex, _ensure_index,\n                                InvalidIndexError)\n import pandas.core.indexing as indexing\n@@ -91,7 +91,7 @@ def _single_replace(self, to_replace, method, inplace, limit):\n     return result\n \n \n-class NDFrame(PandasObject):\n+class NDFrame(PandasObject, SelectionMixin):\n     \"\"\"\n     N-dimensional analogue of DataFrame. Store multi-dimensional in a\n     size-mutable, labeled data structure\n@@ -459,6 +459,16 @@ def size(self):\n         \"\"\"number of elements in the NDFrame\"\"\"\n         return np.prod(self.shape)\n \n+    @property\n+    def _selected_obj(self):\n+        \"\"\" internal compat with SelectionMixin \"\"\"\n+        return self\n+\n+    @property\n+    def _obj_with_exclusions(self):\n+        \"\"\" internal compat with SelectionMixin \"\"\"\n+        return self\n+\n     def _expand_axes(self, key):\n         new_axes = []\n         for k, ax in zip(key, self.axes):\n@@ -2853,6 +2863,66 @@ def pipe(self, func, *args, **kwargs):\n         else:\n             return func(self, *args, **kwargs)\n \n+    _shared_docs['aggregate'] = (\"\"\"\n+    Aggregate using input function or dict of {column ->\n+    function}\n+\n+    .. versionadded:: 0.20.0\n+\n+    Parameters\n+    ----------\n+    func : callable, string, dictionary, or list of string\/callables\n+        Function to use for aggregating the data. If a function, must either\n+        work when passed a DataFrame or when passed to DataFrame.apply. If\n+        passed a dict, the keys must be DataFrame column names.\n+\n+        Accepted Combinations are:\n+        - string function name\n+        - function\n+        - list of functions\n+        - dict of column names -> functions (or list of functions)\n+\n+    Notes\n+    -----\n+    Numpy functions mean\/median\/prod\/sum\/std\/var are special cased so the\n+    default behavior is applying the function along axis=0\n+    (e.g., np.mean(arr_2d, axis=0)) as opposed to\n+    mimicking the default Numpy behavior (e.g., np.mean(arr_2d)).\n+\n+    Returns\n+    -------\n+    aggregated : %(klass)s\n+\n+    See also\n+    --------\n+    \"\"\")\n+\n+    _shared_docs['transform'] = (\"\"\"\n+    Call function producing a like-indexed %(klass)s\n+    and return a %(klass)s with the transformed values`\n+\n+    .. versionadded:: 0.20.0\n+\n+    Parameters\n+    ----------\n+    func : callable, string, dictionary, or list of string\/callables\n+        To apply to column\n+\n+        Accepted Combinations are:\n+        - string function name\n+        - function\n+        - list of functions\n+        - dict of column names -> functions (or list of functions)\n+\n+    Examples\n+    --------\n+    >>> df.transform(lambda x: (x - x.mean()) \/ x.std())\n+\n+    Returns\n+    -------\n+    transformed : %(klass)s\n+    \"\"\")\n+\n     # ----------------------------------------------------------------------\n     # Attribute access\n \n@@ -5990,6 +6060,17 @@ def ewm(self, com=None, span=None, halflife=None, alpha=None,\n \n         cls.ewm = ewm\n \n+        @Appender(_shared_docs['transform'] % _shared_doc_kwargs)\n+        def transform(self, func, *args, **kwargs):\n+            result = self.agg(func, *args, **kwargs)\n+            if is_scalar(result) or len(result) != len(self):\n+                raise ValueError(\"transforms cannot produce \"\n+                                 \"aggregated results\")\n+\n+            return result\n+\n+        cls.transform = transform\n+\n \n def _doc_parms(cls):\n     \"\"\"Return a tuple of the doc parms.\"\"\"\ndiff --git a\/pandas\/core\/series.py b\/pandas\/core\/series.py\n--- a\/pandas\/core\/series.py\n+++ b\/pandas\/core\/series.py\n@@ -2144,6 +2144,49 @@ def map_f(values, f):\n         return self._constructor(new_values,\n                                  index=self.index).__finalize__(self)\n \n+    def _gotitem(self, key, ndim, subset=None):\n+        \"\"\"\n+        sub-classes to define\n+        return a sliced object\n+\n+        Parameters\n+        ----------\n+        key : string \/ list of selections\n+        ndim : 1,2\n+            requested ndim of result\n+        subset : object, default None\n+            subset to act on\n+        \"\"\"\n+        return self\n+\n+    @Appender(generic._shared_docs['aggregate'] % _shared_doc_kwargs)\n+    def aggregate(self, func, axis=0, *args, **kwargs):\n+        axis = self._get_axis_number(axis)\n+        result, how = self._aggregate(func, *args, **kwargs)\n+        if result is None:\n+\n+            # we can be called from an inner function which\n+            # passes this meta-data\n+            kwargs.pop('_axis', None)\n+            kwargs.pop('_level', None)\n+\n+            # try a regular apply, this evaluates lambdas\n+            # row-by-row; however if the lambda is expected a Series\n+            # expression, e.g.: lambda x: x-x.quantile(0.25)\n+            # this will fail, so we can try a vectorized evaluation\n+\n+            # we cannot FIRST try the vectorized evaluation, becuase\n+            # then .agg and .apply would have different semantics if the\n+            # operation is actually defined on the Series, e.g. str\n+            try:\n+                result = self.apply(func, *args, **kwargs)\n+            except (ValueError, AttributeError, TypeError):\n+                result = func(self, *args, **kwargs)\n+\n+        return result\n+\n+    agg = aggregate\n+\n     def apply(self, func, convert_dtype=True, args=(), **kwds):\n         \"\"\"\n         Invoke function on values of Series. Can be ufunc (a NumPy function\n@@ -2167,6 +2210,8 @@ def apply(self, func, convert_dtype=True, args=(), **kwds):\n         See also\n         --------\n         Series.map: For element-wise operations\n+        Series.agg: only perform aggregating type operations\n+        Series.transform: only perform transformating type operations\n \n         Examples\n         --------\n@@ -2244,6 +2289,15 @@ def apply(self, func, convert_dtype=True, args=(), **kwds):\n             return self._constructor(dtype=self.dtype,\n                                      index=self.index).__finalize__(self)\n \n+        # dispatch to agg\n+        if isinstance(func, (list, dict)):\n+            return self.aggregate(func, *args, **kwds)\n+\n+        # if we are a string, try to dispatch\n+        if isinstance(func, compat.string_types):\n+            return self._try_aggregate_string_function(func, *args, **kwds)\n+\n+        # handle ufuncs and lambdas\n         if kwds or args and not isinstance(func, np.ufunc):\n             f = lambda x: func(x, *args, **kwds)\n         else:\n@@ -2253,6 +2307,7 @@ def apply(self, func, convert_dtype=True, args=(), **kwds):\n             if isinstance(f, np.ufunc):\n                 return f(self)\n \n+            # row-wise access\n             if is_extension_type(self.dtype):\n                 mapped = self._values.map(f)\n             else:\n\n<\/patch>",
        "test_patch": ""
      },
      {
        "issue": "MongoDB: Collection-agnostic aggregations like $currentOp doesn't work\nAgent 7.29.1, running on Ubuntu Linux 18.04.\r\n\r\n**Steps to reproduce the issue:**\r\nAdd the following configuration to `\/etc\/datadog-agent\/conf.d\/mongo.d\/conf.yaml` and restart the agent:\r\n```\r\n    custom_queries:\r\n      - metric_prefix: mongodb.custom.queries_slower_than_60sec\r\n        run_on_secondary: true\r\n        query: { \"aggregate\": 1, \"maxTimeMS\": 1000, \"pipeline\": [ { \"$currentOp\": { \"allUsers\": true }}, { \"$match\": { \"active\": true, \"secs_running\": {\"$gt\": 60}}} ], \"cursor\": {}}\r\n        fields:\r\n        - field_name: secs_running\r\n          name: secs_running\r\n          type: gauge\r\n        - field_name: appName\r\n          name: app_name\r\n          type: tag\r\n        - field_name: ns\r\n          name: mongo_op_namespace\r\n          type: tag\r\n```\r\n\r\n**Describe the results you received:**\r\nWhen Datadog attempts to run this command, it produces an error (found via `journalctl`):\r\n```\r\nTraceback (most recent call last):\r\n2021-07-22 06:44:38 UTC | CORE | WARN | (pkg\/collector\/python\/datadog_agent.go:122 in LogMessage) | mongo:375a6f2e54dabf11 | (custom_queries.py:153) | Errors while collecting custom metrics with prefix mongodb.custom.queries_slower_than_60sec\r\nTypeError: name must be an instance of str\r\n    raise TypeError(\"name must be an instance \"\r\n  File \"\/opt\/datadog-agent\/embedded\/lib\/python3.8\/site-packages\/pymongo\/collection.py\", line 160, in __init__\r\n    pymongo.collection.Collection(db, collection_name), result['cursor'], None\r\n  File \"\/opt\/datadog-agent\/embedded\/lib\/python3.8\/site-packages\/datadog_checks\/mongo\/collectors\/custom_queries.py\", line 113, in _collect_custom_metrics_for_query\r\n    self._collect_custom_metrics_for_query(api, raw_query)\r\n  File \"\/opt\/datadog-agent\/embedded\/lib\/python3.8\/site-packages\/datadog_checks\/mongo\/collectors\/custom_queries.py\", line 150, in collect\r\n```\r\n\r\n**Describe the results you expected:**\r\nI would like to be able to send information about slow queries to Datadog.\r\n\r\n**Additional information you deem important (e.g. issue happens only occasionally):**\r\nIt seems like the problem here is that when using this syntax to run an admin aggregation like `$currentOp`, you have to specify `\"aggregate\": 1` in the query to indicate that there is no associated collection. However, the API that Datadog is calling in pymongo expects the collection name to always be a string. Unfortunately, `\"aggregate\": \"1\"` is not equivalent and will fail.\r\n\r\nMore details on the syntax: https:\/\/docs.mongodb.com\/manual\/reference\/command\/aggregate\/\n\n\nHey @atodd-circleci \r\nAcknowledging the limitation, I'm able to reproduce.\r\nI'm thinking we should be able to work around that by putting `$cmd.aggregate` instead of \"1\" as the collection name here: https:\/\/github.com\/DataDog\/integrations-core\/blob\/master\/mongo\/datadog_checks\/mongo\/collectors\/custom_queries.py#L113 but I'd have to confirm that\n@FlorianVeaux Thanks for taking a look so quickly. I manually edited `custom_queries.py` on my installation to replace `collection_name` with the literal `$cmd.aggregate`. It seems to have worked. When I start the agent, I see this in the log:\r\n\r\n```\r\nException: Custom query returned an empty result set.\r\n    raise Exception('Custom query returned an empty result set.')\r\n  File \"\/opt\/datadog-agent\/embedded\/lib\/python3.8\/site-packages\/datadog_checks\/mongo\/collectors\/custom_queries.py\", line 145, in _collect_custom_metrics_for_query\r\n    self._collect_custom_metrics_for_query(api, raw_query)\r\n  File \"\/opt\/datadog-agent\/embedded\/lib\/python3.8\/site-packages\/datadog_checks\/mongo\/collectors\/custom_queries.py\", line 150, in collect\r\nTraceback (most recent call last):\r\n2021-07-27 05:20:05 UTC | CORE | WARN | (pkg\/collector\/python\/datadog_agent.go:122 in LogMessage) | mongo:<redacted> | (custom_queries.py:153) | Errors while collecting custom metrics with prefix mongodb.custom.queries_slower_than_60sec\r\n```\r\n\r\nI'm not expecting any results, so this is good. I can't really go around manually editing our installations this way, though, so I'm looking forward to a more permanent fix.\r\n\r\n(I am a little concerned about having all of these exceptions in the system log, as well. I'll have to look at using [$count](https:\/\/docs.mongodb.com\/manual\/reference\/operator\/aggregation\/count\/) to always output a count instead of what I'm doing now).",
        "patch": "<patch>\ndiff --git a\/mongo\/datadog_checks\/mongo\/collectors\/custom_queries.py b\/mongo\/datadog_checks\/mongo\/collectors\/custom_queries.py\n--- a\/mongo\/datadog_checks\/mongo\/collectors\/custom_queries.py\n+++ b\/mongo\/datadog_checks\/mongo\/collectors\/custom_queries.py\n@@ -56,8 +56,10 @@ def _collect_custom_metrics_for_query(self, api, raw_query):\n         mongo_query = deepcopy(raw_query.get('query'))\n         if not mongo_query:  # no cov\n             raise ValueError(\"Custom query field `query` is required\")\n+        # The mongo command to run (find, aggregate, count...)\n         mongo_command = self._extract_command_from_mongo_query(mongo_query)\n-        collection_name = mongo_query[mongo_command]\n+        # The value of the command, it is usually the collection name on which to run the query.\n+        mongo_command_value = mongo_query[mongo_command]\n         del mongo_query[mongo_command]\n         if mongo_command not in ALLOWED_CUSTOM_QUERIES_COMMANDS:\n             raise ValueError(\"Custom query command must be of type {}\".format(ALLOWED_CUSTOM_QUERIES_COMMANDS))\n@@ -90,20 +92,26 @@ def _collect_custom_metrics_for_query(self, api, raw_query):\n             if field_type not in ALLOWED_CUSTOM_METRICS_TYPES + ['tag']:\n                 raise ValueError('Field `type` must be one of {}'.format(ALLOWED_CUSTOM_METRICS_TYPES + ['tag']))\n \n-        tags = list(tags)\n-        tags.extend(raw_query.get('tags', []))\n-        tags.append('collection:{}'.format(collection_name))\n-\n         try:\n             # This is where it is necessary to extract the command and its argument from the query to pass it as the\n             # first two params.\n-            result = db.command(mongo_command, collection_name, **mongo_query)\n+            result = db.command(mongo_command, mongo_command_value, **mongo_query)\n             if result['ok'] == 0:\n                 raise pymongo.errors.PyMongoError(result['errmsg'])\n         except pymongo.errors.PyMongoError:\n             self.log.error(\"Failed to run custom query for metric %s\", metric_prefix)\n             raise\n \n+        # `1` is Mongo default value for commands that are collection agnostics.\n+        if str(mongo_command_value) == '1':\n+            # https:\/\/github.com\/mongodb\/mongo-python-driver\/blob\/01e34cebdb9aac96c72ddb649e9b0040a0dfd3a0\/pymongo\/aggregation.py#L208\n+            collection_name = '{}.{}'.format(db_name, mongo_command)\n+        else:\n+            collection_name = mongo_command_value\n+\n+        tags.append('collection:{}'.format(collection_name))\n+        tags.extend(raw_query.get('tags', []))\n+\n         if mongo_command == 'count':\n             # A count query simply returns a number, no need to iterate through it.\n             submit_method(metric_prefix, result['n'], tags)\n\n<\/patch>",
        "test_patch": ""
      },
      {
        "issue": "Proposal PIN: Remove ConstantTasks\n# Store Constant Values on Flow\r\n\r\n## Status\r\n\r\nProposed\r\n\r\n## Context\r\n\r\nWhen a dependency is created between a task and any non-task value, Prefect automatically wraps the upstream value in a `ConstantTask` and creates an appropriate edge. This \"works\" in the sense that it properly models a relationship between a computational node (the constant value) and a task that consumes it; but it adds noise to flow graphs that is confusing to users that don't expect it. Furthermore, it adds edge-case potential for error in some circumstance where the constant task fails (for any reason including a node crashing) and results in a difficult-to-diagnose flow failure. In addition, it creates difficulties with triggers, as the `ConstantTask` always succeeds.\r\n\r\nSimply put: ConstantTasks are confusing and do not add information.\r\n\r\n## Proposal\r\n\r\nConstantTasks will no longer be automatically created (though they can stay in the task library, as they may be useful for particularly large objects that users only want to reference once in their flow). \r\n\r\nInstead, if a task is attached to any constant (non-Task) value, that value is added to a new flow attribute, `flow.constant_values`. This attribute is a dictionary `Dict[Task, Dict[str, Any]]` that maps each task to a collection of input argument values. For example, if task `t` is called in the functional API as `t(x=1)`, then `flow.constants[t]` would be updated to `{t: {x: 1}}`. An additional \"validation\" step would ensure that the keys of `flow.constants` were always a strict subset of `flow.tasks`. \r\n\r\nIn addition, the `TaskRunner.run()` method will accept a `constant_inputs` kwarg and `FlowRunner.run()` will read and provide the appropriate values for each task. The TaskRunner is responsible for combining those with the upstream state values in order to provide all input arguments for the task itself.\r\n\r\n## Consequences\r\n\r\n`ConstantTasks` will no longer be created (though they can remain in existence in case users find them useful for encapsulating some piece of information). Constants that are used in a flow will be stored in a nested dictionary attached to the flow itself.\r\n\n\n\nThis is half of a PIN. I don't immediately have a great solution for the problem of where constants should be tracked, if not in the flow graph itself. However, `ConstantTasks` are annoying enough that they \"feel\" like a problem we can wish out of existence. Would love to hear some thoughts about different approaches.\n> I don't think it's straightforward to attach the constant values to the task, since a task object is not necessarily bound to a specific flow (and therefore could have different constant inputs in two different flows). \r\n\r\nJust out of curiosity, do we really expect users to define two flows in the same file \/ process?\nExpect, no.\r\n\r\nAnticipate, maybe. \r\n\r\nWe've very successfully adhered to the \"flowless task\" model thus far and I don't want to abandon it quite yet!\nSo, if we are 100% sure don't want to attach this data to the Task object itself, it necessarily must be attached to the Flow, in which case we would need three pieces of information:\r\n- the task\r\n- the keyword argument\r\n- the constant value\r\n\r\nPossibly `flow.constants: Dict[Task, Dict[str, Any]]` would work here?  And then somehow feed that information to the task runner via a `constant_inputs` kwarg or something like that?\nI think you're right, if it's not attached to the task it must be the flow!\r\n\r\nSo:\r\n- `flow.constants` could be, for example, `{task: {\"x\": 1}}`\r\n- The `FlowRunner` would check `flow.constants` when building the task's arguments, and simply include the constant value under the appropriate kwarg. In this case, `task` would receive `x=1`.\r\n- we'd need an additional validation check that `constant.keys()` was a strict subset of `flow.tasks`.\nOh, I missed your finer point about why we need `constant_inputs` -- 1 has no `State`, and if we give it a state it'll fail some triggers.\r\n\r\nI'm not crazy about a separate `constant_inputs` because it feels like tasks become much harder to run without a TaskRunner to put everything in the right place, but I'm not sure I see a solution immediately.\nI've updated the PIN proposal to incorporate this discussion",
        "patch": "<patch>\ndiff --git a\/src\/prefect\/utilities\/notifications.py b\/src\/prefect\/utilities\/notifications.py\n--- a\/src\/prefect\/utilities\/notifications.py\n+++ b\/src\/prefect\/utilities\/notifications.py\n@@ -1,7 +1,7 @@\n \"\"\"\n Tools and utilities for notifications and callbacks.\n \n-For an in-depth guide to setting up your system for using Slack notifications, [please see our tutorial](..\/..\/guide\/tutorials\/slack-notifications.html).\n+For an in-depth guide to setting up your system for using Slack notifications, [please see our tutorial](\/guide\/tutorials\/slack-notifications.html).\n \"\"\"\n import smtplib\n from email.header import Header\n\n<\/patch>",
        "test_patch": ""
      }
    ],
    "signature_instructions": "\n    Transforms software issues into actionable patches. In the style of a FAANG\n    System Architect interview question solution.\n    ",
    "signature_prefix": "Patch:",
    "extended_signature_instructions": "\n    Transforms software issues into actionable patches. In the style of a FAANG\n    System Architect interview question solution.\n    ",
    "extended_signature_prefix": "Patch:"
  }
}