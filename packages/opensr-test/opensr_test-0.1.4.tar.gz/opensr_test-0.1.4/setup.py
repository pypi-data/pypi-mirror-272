# -*- coding: utf-8 -*-
from setuptools import setup

packages = \
['opensr_test']

package_data = \
{'': ['*']}

install_requires = \
['kornia>=0.7.2',
 'matplotlib>=3.7.0',
 'numpy>=1.25.2',
 'opencv-python>=4.8.0.0',
 'pydantic>=2.7.1',
 'requests>=2.25.0',
 'satalign>=0.1.0',
 'scikit-image>=0.19.0',
 'torchvision>=0.17.0']

setup_kwargs = {
    'name': 'opensr-test',
    'version': '0.1.4',
    'description': 'A comprehensive benchmark for real-world Sentinel-2 imagery super-resolution',
    'long_description': '<p align="center">\n  <a href="https://github.com/ESAOpenSR/opensr-test"><img src="docs/images/logo.png" alt="header" width="55%"></a>\n</p>\n\n<p align="center">\n    <em>A comprehensive benchmark for real-world Sentinel-2 imagery super-resolution</em>\n</p>\n\n<p align="center">\n<a href=\'https://pypi.python.org/pypi/opensr-test\'>\n    <img src=\'https://img.shields.io/pypi/v/opensr-test.svg\' alt=\'PyPI\' />\n</a>\n\n<a href="https://opensource.org/licenses/MIT" target="_blank">\n    <img src="https://img.shields.io/badge/License-MIT-blue.svg" alt="License">\n</a>\n<a href="https://github.com/psf/black" target="_blank">\n    <img src="https://img.shields.io/badge/code%20style-black-000000.svg" alt="Black">\n</a>\n<a href="https://pycqa.github.io/isort/" target="_blank">\n    <img src="https://img.shields.io/badge/%20imports-isort-%231674b1?style=flat&labelColor=ef8336" alt="isort">\n</a>\n</p>\n\n---\n\n**GitHub**: [https://github.com/ESAOpenSR/opensr-test](https://github.com/ESAOpenSR/opensr-test)\n\n**Documentation**: [https://esaopensr.github.io/opensr-test](https://esaopensr.github.io/opensr-test)\n\n**PyPI**: [https://pypi.org/project/opensr-test/](https://pypi.org/project/opensr-test/)\n\n**Paper**: https://www.techrxiv.org/users/760184/articles/735467-a-comprehensive-benchmark-for-optical-remote-sensing-image-super-resolution\n\n---\n\n#\n\n## Overview\n\nSuper-Resolution (SR) aims to improve satellite imagery ground sampling distance. However, two problems are common in the literature. First, most models are **tested on synthetic data**, raising doubts about their real-world applicability and performance. Second, traditional evaluation metrics such as PSNR, LPIPS, and SSIM are not designed to assess SR performance. These metrics fall short, especially in conditions involving changes in luminance or spatial misalignments - scenarios frequently encountered in real world.\n\nTo address these challenges, \'opensr-test\' provides a fair approach for SR benchmark. We provide three datasets carefully crafted to minimize spatial and spectral misalignment. Besides, \'opensr-test\' precisely assesses SR algorithm performance across three independent metrics groups that measure consistency, synthesis, and correctness.\n\n<p align="center">\n  <img src="docs/images/diagram.png" alt="header">\n</p>\n\n## How to use\n\nThe example below shows how to use `opensr-test` to benchmark your SR model.\n\n\n```python\nimport torch\nimport opensr_test\n\nlr = torch.rand(4, 64, 64)\nhr = torch.rand(4, 256, 256)\nsr = torch.rand(4, 256, 256)\n\nmetrics = opensr_test.Metrics()\nmetrics.compute(lr=lr, sr=sr, hr=hr)\n```\n\n## Benchmark\n\nBenchmark comparison of SR models. Downward arrows (↓) denote metrics in which lower values are preferable, and upward arrows (↑) indicate metrics in which higher values reflect better performance.\n\n<table border="1" style="width:75%; text-align:center;">    \n    <thead>\n        <tr>\n            <th colspan="2"></th>\n            <th colspan="3">Consistency</th>\n            <th>Synthesis</th>\n            <th colspan="3">Correctness</th>\n        </tr>\n        <tr>\n            <th colspan="2"></th>\n            <th>reflectance ↓</th>\n            <th>spectral ↓</th>\n            <th>spatial ↓</th>\n            <th>high-frequency ↑</th>\n            <th>ha ↓</th>\n            <th>om ↓</th>\n            <th>im ↑</th>\n        </tr>\n    </thead>\n    <tbody>\n        <tr>\n            <td rowspan="3"><b>NAIP</b></td>\n            <td><b>SuperImage</b></td>\n            <td><b>0.008</b></td>\n            <td><b>7.286</b></td>\n            <td><b>0.131</b></td>\n            <td>0.003</td>\n            <td><b>0.117</b></td>\n            <td><b>0.784</b></td>\n            <td><b>0.098</b></td>\n        </tr>\n        <tr>\n            <td><b>SR4RS</b></td>\n            <td>0.016</td>\n            <td><b>3.471</b></td>\n            <td>1.156</td>\n            <td>0.010</td>\n            <td>0.869</td>\n            <td>0.077</td>\n            <td>0.054</td>\n        </tr>\n        <tr>\n            <td><b>diffusers</b></td>\n            <td>0.463</td>\n            <td>12.437</td>\n            <td>2.88</td>\n            <td><b>0.013</b></td>\n            <td>0.905</td>\n            <td>0.055</td>\n            <td>0.040</td>\n        </tr>\n        <tr>\n            <td rowspan="3"><b>SPOT</b></td>\n            <td><b>SuperImage</b></td>\n            <td><b>0.009</b></td>\n            <td>3.512</td>\n            <td><b>0.062</b></td>\n            <td>0.006</td>\n            <td><b>0.160</b></td>\n            <td><b>0.794</b></td>\n            <td><b>0.046</b></td>\n        </tr>\n        <tr>\n            <td><b>SR4RS</b></td>\n            <td>0.039</td>\n            <td><b>3.232</b></td>\n            <td>1.151</td>\n            <td><b>0.023</b></td>\n            <td>0.834</td>\n            <td>0.115</td>\n            <td>0.051</td>\n        </tr>\n        <tr>\n            <td><b>diffusers</b></td>\n            <td>0.417</td>\n            <td>11.730</td>\n            <td>0.817</td>\n            <td>0.014</td>\n            <td>0.686</td>\n            <td>0.251</td>\n            <td>0.063</td>\n        </tr>\n        <tr>\n            <td rowspan="3"><b>VENµS</b></td>\n            <td><b>SuperImage</b></td>\n            <td><b>0.009</b></td>\n            <td>8.687</td>\n            <td><b>0.099</b></td>\n            <td>0.003</td>\n            <td><b>0.403</b></td>\n            <td><b>0.380</b></td>\n            <td><b>0.217</b></td>\n        </tr>\n        <tr>\n            <td><b>SR4RS</b></td>\n            <td>0.014</td>\n            <td><b>3.394</b></td>\n            <td>1.122</td>\n            <td><b>0.012</b></td>\n            <td>0.971</td>\n            <td>0.017</td>\n            <td>0.012</td>\n        </tr>\n        <tr>\n            <td><b>diffusers</b></td>\n            <td>0.467</td>\n            <td>13.303</td>\n            <td>0.806</td>\n            <td>0.009</td>\n            <td>0.933</td>\n            <td>0.043</td>\n            <td>0.024</td>\n        </tr>\n    </tbody>\n</table>\n\n## Installation\n\nInstall the latest version from PyPI:\n\n```\npip install opensr-test\n```\n\nUpgrade `opensr-test` by running:\n\n```\npip install -U opensr-test\n```\n\nInstall the latest dev version from GitHub by running:\n\n```\npip install git+https://github.com/ESAOpenSR/opensr-test\n```\n\n## Examples\n\nThe following examples show how to use `opensr-test` to benchmark your SR model.\n\n- Use `opensr-test` with TensorFlow model [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1cAGDGlj5Kqt343inNni3ByLE1856z0gE#scrollTo=xaivkcD5Zfw1&uniqifier=1)\n\n- Use `opensr-test` with PyTorch model [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1Db8-JSMTF-hNZQv2UyBDclxkO5hgP9VR#scrollTo=jVL7o6yOrJkY)\n\n- Use `opensr-test` with a diffuser model [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1banDovG43c2OBh9MODPN4OXgaSCXu1Dc#scrollTo=zz4Aw7_52ulT)\n\n## Visualizations\n\nThe `opensr-test` package provides a set of visualizations to help you understand the performance of your SR model.\n\n```python\nimport torch\nimport opensr_test\nimport matplotlib.pyplot as plt\n\nfrom super_image import HanModel\n\n# Define the SR model\nsrmodel = HanModel.from_pretrained(\'eugenesiow/han\', scale=4)\n\n# Load the data\nlr, hr, landuse, parameters = opensr_test.load("spot").values()\n\n# Define the benchmark experiment\nmetrics = opensr_test.Metrics()\n\n# Define the image to be tested\nidx = 0\nlr_img = torch.from_numpy(lr[idx, 0:3])\nhr_img = torch.from_numpy(hr[idx, 0:3])\nsr_img = srmodel(lr_img[None]).squeeze().detach()\n\n# Compute the metrics\nmetrics.compute(\n    lr=lr_img, sr=sr_img, hr=hr_img,\n    stability_threshold = parameters.stability_threshold[idx],\n    im_score = parameters.correctness_params[0],\n    om_score = parameters.correctness_params[1],\n    ha_score = parameters.correctness_params[2]\n)\n```\n\nNow, we can visualize the results using the `opensr_test.visualize` module.\nfDisplay the triplets LR, SR and HR images:\n\n```python\nmetrics.plot_triplets()\n```\n\n<p align="center">\n  <img src="docs/images/example01.png">\n</p>\n\nDisplay the quadruplets LR, SR, HR and landuse images:\n\n```python\nmetrics.plot_quadruplets()\n```\n\n<p align="center">\n  <img src="docs/images/example02.png">\n</p>\n\n\nDisplay the matching points between the LR and SR images:\n\n```python\nmetrics.plot_spatial_matches()\n```\n\n<p align="center">\n  <img src="docs/images/example03.png" width="70%">\n</p>\n\n\nDisplay a summary of all the metrics:\n\n```python\nmetrics.plot_summary()\n```\n\n<p align="center">\n  <img src="docs/images/example04.png">\n</p>\n\n\nDisplay the correctness of the SR image:\n\n```python\nmetrics.plot_tc()\n```\n\n<p align="center">\n  <img src="docs/images/example05.png">\n</p>\n\n## Deeper understanding\n\nExplore the [API](https://esaopensr.github.io/opensr-test/docs/API/config_pydantic.html) section for more details about personalizing your benchmark experiments.\n\n<p align="center">\n    <a href="/docs/api.md"><img src="docs/images/image02.png" alt="opensr-test" width="30%"></a>\n</p>\n\n## Citation\n\nIf you use `opensr-test` in your research, please cite our paper:\n\n```\n@article{aybar2024comprehensive,\n  title={A Comprehensive Benchmark for Optical Remote Sensing Image Super-Resolution},\n  author={Aybar, Cesar and Montero, David and Donike, Simon and Kalaitzis, Freddie and G{\\\'o}mez-Chova, Luis},\n  journal={Authorea Preprints},\n  year={2024},\n  publisher={Authorea}\n}\n\n```\n\n## Acknowledgements\n\nThis work was make with the support of the European Space Agency (ESA) under the project “Explainable AI: application to trustworthy super-resolution (OpenSR)”. Cesar Aybar acknowledges support by the National Council of Science, Technology, and Technological Innovation (CONCYTEC, Peru) through the “PROYECTOS DE INVESTIGACIÓN BÁSICA – 2023-01” program with contract number PE501083135-2023-PROCIENCIA. Luis Gómez-Chova acknowledges support from the Spanish Ministry of Science and Innovation (project PID2019-109026RB-I00 funded by MCIN/AEI/10.13039/501100011033).\n',
    'author': 'Cesar Aybar',
    'author_email': 'cesar.aybar@uv.es',
    'maintainer': 'None',
    'maintainer_email': 'None',
    'url': 'https://github.com/csaybar/opensr-test',
    'packages': packages,
    'package_data': package_data,
    'install_requires': install_requires,
    'python_requires': '>=3.10,<4.0',
}


setup(**setup_kwargs)
