"use strict";(self.webpackChunk_jupyterlab_transformers_completer=self.webpackChunk_jupyterlab_transformers_completer||[]).push([[468],{468:(e,t,o)=>{o.r(t),o.d(t,{default:()=>m});var s=o(138),i=o(712),r=o(930);function n(e,t=2,o=1024){if(0===e)return"0 B";const s=t,i=["B","KB","MB","GB","TB","PB","EB","ZB","YB"],r=Math.floor(Math.log(e)/Math.log(o));return r>=0&&r<i.length?parseFloat((e/Math.pow(o,r)).toFixed(s))+" "+i[r]:String(e)}const a=[{repo:"Xenova/tiny_starcoder_py",licence:"bigcode-openrail-m",humanEval:7.84},{repo:"Xenova/codegen-350M-mono",licence:"bsd-3-clause"},{repo:"Xenova/codegen-350M-multi",licence:"bsd-3-clause"},{repo:"Xenova/starcoderbase-1b-sft",licence:"???",humanEval:39},{repo:"Xenova/WizardCoder-1B-V1.0",licence:"bigcode-openrail-m",humanEval:23.8},{repo:"Xenova/J-350M",licence:"bsd-3-clause"}],l=[{repo:"Xenova/gpt2",licence:"mit"},{repo:"Xenova/TinyLLama-v0",licence:"apache-2.0"},{repo:"Xenova/dlite-v2-774m",licence:"apache-2.0"},{repo:"Xenova/LaMini-GPT-124M",licence:"cc-by-nc-4.0"},{repo:"Xenova/LaMini-Cerebras-111M",licence:"cc-by-nc-4.0"},{repo:"Xenova/LaMini-Cerebras-256M",licence:"cc-by-nc-4.0"},{repo:"Xenova/LaMini-Cerebras-590M",licence:"cc-by-nc-4.0"},{repo:"Xenova/opt-125m",licence:"other"},{repo:"Xenova/pythia-70m-deduped",licence:"apache-2.0"},{repo:"Xenova/distilgpt2",licence:"apache-2.0"},{repo:"Xenova/llama-160m",licence:"other"},{repo:"Xenova/Qwen1.5-0.5B-Chat",licence:"tongyi-qianwen-research"},{repo:"Xenova/bloom-560m",licence:"bigscience-bloom-rail-1.0"}],d={codeModel:"Xenova/tiny_starcoder_py",textModel:"Xenova/gpt2",temperature:.5,doSample:!1,topK:5,maxNewTokens:50,generateN:2,maxContextWindow:512,diversityPenalty:1,repetitionPenalty:1,allowLocalModels:!1,allowRemoteModels:!0,remoteHost:"https://huggingface.co/",localModelPath:"/models/"};class c{constructor(e){this.options=e,this.identifier="@krassowski/inline-completer",this.name="Transformers powered completions",this._currentGeneration=0,this._currentModels={},this._loadingNotifications={},this._ready={},this._settings=d,this._streamPromises=new Map,this._tokenCounter=0,this._workerStarted=new r.PromiseDelegate;try{SharedArrayBuffer}catch(e){(0,i.showErrorMessage)("SharedArrayBuffer not available","Server extension enabling `same-origin` and `require-corp` headers is required for jupyterlab-transformers-completer to access `SharedArrayBuffer` which is used to synchronously communicate with the language model WebWorker.")}const t=new SharedArrayBuffer(1024);this._sharedArray=new Int32Array(t),e.worker.addEventListener("message",this._onMessageReceived.bind(this)),this._workerStarted.promise.then((()=>{this._postMessage({action:"initializeBuffer",buffer:t})}))}get schema(){return{properties:{codeModel:{title:"Code model",description:"Model used in code cells and code files.",oneOf:[{const:"none",title:"No model"},...a.map(this._formatModelOptions)],type:"string"},textModel:{title:"Text model",description:"Model used in Markdown (cells and files) and plain text files.",oneOf:[{const:"none",title:"No model"},...l.map(this._formatModelOptions)],type:"string"},temperature:{minimum:0,maximum:1,type:"number",title:"Temperature",description:"The value used to module the next token probabilities."},doSample:{type:"boolean",description:"Whether to use sampling; use greedy decoding otherwise."},topK:{minimum:0,maximum:50,type:"number",title:"Top k",description:"The number of highest probability vocabulary tokens to keep for top-k-filtering."},maxNewTokens:{minimum:1,maximum:512,type:"number",title:"Tokens limit",description:"Maximum number of new tokens."},generateN:{minimum:1,type:"number",title:"Candidates",description:"How many completion candidates should be generated."},diversityPenalty:{type:"number",title:"Diversity penalty",description:"1.0 means not penalty."},repetitionPenalty:{type:"number",title:"Repetition penalty",description:"1.0 means not penalty."},maxContextWindow:{title:"Context window",minimum:1,type:"number",description:"At most how many characters should be provided to the model. Smaller context results in faster generation at a cost of less accurate suggestions."},allowLocalModels:{type:"boolean",title:"Check Local Model Database",default:!1,description:"Whether to allow downloading models from local database of models. If set to false (default) the models will be downloaded from remote host."},allowRemoteModels:{type:"boolean",title:"Check Remote Model Database",default:!0,description:"Whether to allow downloading models from remote database of models."},localModelPath:{type:"string",title:"Local Model Database Path",default:"/models/"},remoteHost:{type:"string",title:"Remote Host",description:"The remote host from which to download the models.",default:"https://huggingface.co/"}},default:d}}async configure(e){this._settings=e,await this._workerStarted.promise,this._postMessage({action:"configure",allowLocalModels:this._settings.allowLocalModels,allowRemoteModels:this._settings.allowRemoteModels,localModelPath:this._settings.localModelPath,remoteHost:this._settings.remoteHost}),this._switchModel(this._settings.codeModel,"code"),this._switchModel(this._settings.textModel,"text")}async fetch(e,t){const o=["text/x-ipythongfm","text/x-markdown","text/plain","text/x-rst","text/x-latex","text/x-rsrc"].includes(e.mimeType)?this._settings.textModel:this._settings.codeModel;await this._ready[o].promise,this._abortPrevious(),this._streamPromises=new Map;const s=this._prefixFromRequest(e),i=[],r=[];for(let e=0;e<this._settings.generateN;e++){const e="T"+ ++this._tokenCounter;r.push(e),i.push({insertText:"",isIncomplete:!0,token:e})}return this._postMessage({model:o,text:s,maxNewTokens:this._settings.maxNewTokens,temperature:this._settings.temperature,topK:this._settings.topK,doSample:this._settings.doSample,generateN:this._settings.generateN,repetitionPenalty:this._settings.repetitionPenalty,diversityPenalty:this._settings.diversityPenalty,idTokens:r,action:"generate",counter:this._currentGeneration}),{items:i}}async*stream(e){let t=!1;for(;!t;){const o=new r.PromiseDelegate;this._streamPromises.set(e,o);const s=o.promise;yield s,t=(await s).done}}_onMessageReceived(e){const t=e.data;switch(t.status){case"worker-started":this._msgWorkerStarted(t);break;case"initiate":this._msgInitiate(t);break;case"progress":this._msgProgress(t);break;case"done":this._msgDone(t);break;case"ready":this._msgReady(t);break;case"update":this._msgUpdate(t);break;case"complete":this._msgComplete(t);break;case"interrupted":this._msgInterrupted(t);break;case"exception":this._msgException(t)}}_msgWorkerStarted(e){this._workerStarted.resolve(void 0)}_msgInitiate(e){this._ready[e.model]=new r.PromiseDelegate;const t=`Loading ${e.model}: ${e.file}`;this._loadingNotifications[e.model]?i.Notification.update({id:this._loadingNotifications[e.model],message:t,autoClose:!1}):this._loadingNotifications[e.model]=i.Notification.emit(t,"in-progress",{autoClose:!1})}_msgProgress(e){i.Notification.update({id:this._loadingNotifications[e.model],message:`Loading ${e.model}: ${e.file} ${Math.round(e.progress)}% (${n(e.loaded,1)}/${n(e.total)})`,type:"in-progress",autoClose:!1,progress:e.progress/100})}_msgDone(e){i.Notification.update({id:this._loadingNotifications[e.model],message:`Loaded ${e.file} for ${e.model}, compiling...`,type:"success",autoClose:!1})}_msgReady(e){i.Notification.dismiss(this._loadingNotifications[e.model]),this._ready[e.model].resolve(void 0)}_msgUpdate(e){this._tickWorker();const t=e.idToken,o=this._streamPromises.get(t);o?o.resolve({done:!1,response:{insertText:e.output}}):console.warn("Completion updated but stream absent")}_msgComplete(e){const t=e.idToken,o=this._streamPromises.get(t);o?o.resolve({done:!0,response:{insertText:e.output}}):console.warn("Completion done but stream absent"),this._streamPromises.delete(t)}_msgInterrupted(e){for(const t of e.idTokens){const e=this._streamPromises.get(t);e&&e.reject(null),this._streamPromises.delete(t)}}_msgException(e){var t;i.Notification.error(`Worker error: ${null===(t=e.error)||void 0===t?void 0:t.message}`),console.error(e)}_formatModelOptions(e){const t=e.repo.replace("Xenova/","");return{const:e.repo,title:`${t} (${e.licence})`}}_tickWorker(){Atomics.store(this._sharedArray,0,this._currentGeneration),Atomics.notify(this._sharedArray,0,1)}_abortPrevious(){this._currentGeneration++,this._tickWorker()}_prefixFromRequest(e){const t=e.text.slice(0,e.offset);return t.slice(-Math.min(this._settings.maxContextWindow,t.length))}_postMessage(e){this.options.worker.postMessage(e)}_switchModel(e,t){const o=this._currentModels[t];o!==e&&(o&&this._postMessage({action:"disposeModel",model:o}),"none"!==e&&this._postMessage({action:"initializeModel",model:e}),this._currentModels[t]=e)}}const m={id:"@jupyterlab/transformers-completer:plugin",description:"An in-browser AI completion provider for JupyterLab.",requires:[s.ICompletionProviderManager],autoStart:!0,activate:(e,t)=>{const s=new Worker(new URL(o.p+o.u(213),o.b)),i=new c({worker:s});t.registerInlineProvider(i)}}}}]);