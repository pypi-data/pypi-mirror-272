# This file was auto-generated by Fern from our API Definition.

import typing
import urllib.parse
from json.decoder import JSONDecodeError

from ..core.api_error import ApiError
from ..core.client_wrapper import AsyncClientWrapper, SyncClientWrapper
from ..core.datetime_utils import serialize_datetime
from ..core.jsonable_encoder import jsonable_encoder
from ..core.pydantic_utilities import pydantic_v1
from ..core.remove_none_from_dict import remove_none_from_dict
from ..core.request_options import RequestOptions
from ..errors.errors.invalid_staged_file_error import InvalidStagedFileError
from ..errors.errors.object_not_found_error import ObjectNotFoundError
from ..errors.errors.redirect import Redirect
from ..errors.types.invalid_staged_file_body import InvalidStagedFileBody
from ..errors.types.object_not_found_body import ObjectNotFoundBody
from ..external_tasks.types.initial_task_response import InitialTaskResponse
from ..pagination.errors.pagination_error import PaginationError
from ..pagination.types.created_gt import CreatedGt
from ..pagination.types.created_lt import CreatedLt
from ..pagination.types.limit import Limit
from ..pagination.types.next_token import NextToken
from ..pagination.types.sort import Sort
from ..pagination.types.updated_gt import UpdatedGt
from ..pagination.types.updated_lt import UpdatedLt
from ..shared.dataset.types.dataset_id import DatasetId
from .errors.non_csv_file_error import NonCsvFileError
from .types.create_dataset import CreateDataset
from .types.paginated_datasets import PaginatedDatasets

# this is used as the default value for optional parameters
OMIT = typing.cast(typing.Any, ...)


class DatasetsClient:
    def __init__(self, *, client_wrapper: SyncClientWrapper):
        self._client_wrapper = client_wrapper

    def list_datasets(
        self,
        *,
        limit: typing.Optional[Limit] = None,
        sort: typing.Optional[Sort] = None,
        created_at_gt: typing.Optional[CreatedGt] = None,
        created_at_lt: typing.Optional[CreatedLt] = None,
        updated_at_gt: typing.Optional[UpdatedGt] = None,
        updated_at_lt: typing.Optional[UpdatedLt] = None,
        next_token: typing.Optional[NextToken] = None,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> PaginatedDatasets:
        """
        Parameters
        ----------
        limit : typing.Optional[Limit]

        sort : typing.Optional[Sort]

        created_at_gt : typing.Optional[CreatedGt]

        created_at_lt : typing.Optional[CreatedLt]

        updated_at_gt : typing.Optional[UpdatedGt]

        updated_at_lt : typing.Optional[UpdatedLt]

        next_token : typing.Optional[NextToken]

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        PaginatedDatasets

        Examples
        --------
        from sphinxbio.client import Sphinxbio

        client = Sphinxbio(
            token="YOUR_TOKEN",
        )
        client.datasets.list_datasets()
        """
        _response = self._client_wrapper.httpx_client.request(
            method="GET",
            url=urllib.parse.urljoin(f"{self._client_wrapper.get_base_url()}/", "v1/datasets"),
            params=jsonable_encoder(
                remove_none_from_dict(
                    {
                        "limit": limit,
                        "sort": sort,
                        "createdAt.gt": serialize_datetime(created_at_gt) if created_at_gt is not None else None,
                        "createdAt.lt": serialize_datetime(created_at_lt) if created_at_lt is not None else None,
                        "updatedAt.gt": serialize_datetime(updated_at_gt) if updated_at_gt is not None else None,
                        "updatedAt.lt": serialize_datetime(updated_at_lt) if updated_at_lt is not None else None,
                        "nextToken": next_token,
                        **(
                            request_options.get("additional_query_parameters", {})
                            if request_options is not None
                            else {}
                        ),
                    }
                )
            ),
            headers=jsonable_encoder(
                remove_none_from_dict(
                    {
                        **self._client_wrapper.get_headers(),
                        **(request_options.get("additional_headers", {}) if request_options is not None else {}),
                    }
                )
            ),
            timeout=request_options.get("timeout_in_seconds")
            if request_options is not None and request_options.get("timeout_in_seconds") is not None
            else self._client_wrapper.get_timeout(),
            retries=0,
            max_retries=request_options.get("max_retries") if request_options is not None else 0,  # type: ignore
        )
        try:
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        if 200 <= _response.status_code < 300:
            return pydantic_v1.parse_obj_as(PaginatedDatasets, _response_json)  # type: ignore
        if "error" in _response_json:
            if _response_json["error"] == "PaginationError":
                raise PaginationError(pydantic_v1.parse_obj_as(str, _response_json["content"]))  # type: ignore
        raise ApiError(status_code=_response.status_code, body=_response_json)

    def download_dataset(
        self, dataset_id: DatasetId, *, request_options: typing.Optional[RequestOptions] = None
    ) -> str:
        """
        Parameters
        ----------
        dataset_id : DatasetId

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        str

        Examples
        --------
        from sphinxbio.client import Sphinxbio

        client = Sphinxbio(
            token="YOUR_TOKEN",
        )
        client.datasets.download_dataset(
            dataset_id="ds_12345",
        )
        """
        _response = self._client_wrapper.httpx_client.request(
            method="GET",
            url=urllib.parse.urljoin(
                f"{self._client_wrapper.get_base_url()}/", f"v1/datasets/{jsonable_encoder(dataset_id)}/download"
            ),
            params=jsonable_encoder(
                request_options.get("additional_query_parameters") if request_options is not None else None
            ),
            headers=jsonable_encoder(
                remove_none_from_dict(
                    {
                        **self._client_wrapper.get_headers(),
                        **(request_options.get("additional_headers", {}) if request_options is not None else {}),
                    }
                )
            ),
            timeout=request_options.get("timeout_in_seconds")
            if request_options is not None and request_options.get("timeout_in_seconds") is not None
            else self._client_wrapper.get_timeout(),
            retries=0,
            max_retries=request_options.get("max_retries") if request_options is not None else 0,  # type: ignore
        )
        try:
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        if 200 <= _response.status_code < 300:
            return pydantic_v1.parse_obj_as(str, _response_json)  # type: ignore
        if "error" in _response_json:
            if _response_json["error"] == "Redirect":
                raise Redirect()
            if _response_json["error"] == "ObjectNotFoundError":
                raise ObjectNotFoundError(
                    pydantic_v1.parse_obj_as(ObjectNotFoundBody, _response_json["content"])  # type: ignore
                )
        raise ApiError(status_code=_response.status_code, body=_response_json)

    def create_dataset(
        self, *, request: CreateDataset, request_options: typing.Optional[RequestOptions] = None
    ) -> InitialTaskResponse:
        """
        Parameters
        ----------
        request : CreateDataset

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        InitialTaskResponse

        Examples
        --------
        from sphinxbio import CreateDataset
        from sphinxbio.client import Sphinxbio

        client = Sphinxbio(
            token="YOUR_TOKEN",
        )
        client.datasets.create_dataset(
            request=CreateDataset(
                campaign_id="camp_67890",
                description="Dataset containing gene expression levels.",
                eln_link="https://eln.example.com",
                name="qPCR Results",
                staged_upload_path="unique-prefix/results.csv",
                tags=["exp001", "Gene Expression", "ML Ready"],
            ),
        )
        """
        _response = self._client_wrapper.httpx_client.request(
            method="POST",
            url=urllib.parse.urljoin(f"{self._client_wrapper.get_base_url()}/", "v1/datasets"),
            params=jsonable_encoder(
                request_options.get("additional_query_parameters") if request_options is not None else None
            ),
            json=jsonable_encoder(request)
            if request_options is None or request_options.get("additional_body_parameters") is None
            else {
                **jsonable_encoder(request),
                **(jsonable_encoder(remove_none_from_dict(request_options.get("additional_body_parameters", {})))),
            },
            headers=jsonable_encoder(
                remove_none_from_dict(
                    {
                        **self._client_wrapper.get_headers(),
                        **(request_options.get("additional_headers", {}) if request_options is not None else {}),
                    }
                )
            ),
            timeout=request_options.get("timeout_in_seconds")
            if request_options is not None and request_options.get("timeout_in_seconds") is not None
            else self._client_wrapper.get_timeout(),
            retries=0,
            max_retries=request_options.get("max_retries") if request_options is not None else 0,  # type: ignore
        )
        try:
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        if 200 <= _response.status_code < 300:
            return pydantic_v1.parse_obj_as(InitialTaskResponse, _response_json)  # type: ignore
        if "error" in _response_json:
            if _response_json["error"] == "NonCSVFileError":
                raise NonCsvFileError(pydantic_v1.parse_obj_as(str, _response_json["content"]))  # type: ignore
            if _response_json["error"] == "InvalidStagedFileError":
                raise InvalidStagedFileError(
                    pydantic_v1.parse_obj_as(InvalidStagedFileBody, _response_json["content"])  # type: ignore
                )
        raise ApiError(status_code=_response.status_code, body=_response_json)


class AsyncDatasetsClient:
    def __init__(self, *, client_wrapper: AsyncClientWrapper):
        self._client_wrapper = client_wrapper

    async def list_datasets(
        self,
        *,
        limit: typing.Optional[Limit] = None,
        sort: typing.Optional[Sort] = None,
        created_at_gt: typing.Optional[CreatedGt] = None,
        created_at_lt: typing.Optional[CreatedLt] = None,
        updated_at_gt: typing.Optional[UpdatedGt] = None,
        updated_at_lt: typing.Optional[UpdatedLt] = None,
        next_token: typing.Optional[NextToken] = None,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> PaginatedDatasets:
        """
        Parameters
        ----------
        limit : typing.Optional[Limit]

        sort : typing.Optional[Sort]

        created_at_gt : typing.Optional[CreatedGt]

        created_at_lt : typing.Optional[CreatedLt]

        updated_at_gt : typing.Optional[UpdatedGt]

        updated_at_lt : typing.Optional[UpdatedLt]

        next_token : typing.Optional[NextToken]

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        PaginatedDatasets

        Examples
        --------
        from sphinxbio.client import AsyncSphinxbio

        client = AsyncSphinxbio(
            token="YOUR_TOKEN",
        )
        await client.datasets.list_datasets()
        """
        _response = await self._client_wrapper.httpx_client.request(
            method="GET",
            url=urllib.parse.urljoin(f"{self._client_wrapper.get_base_url()}/", "v1/datasets"),
            params=jsonable_encoder(
                remove_none_from_dict(
                    {
                        "limit": limit,
                        "sort": sort,
                        "createdAt.gt": serialize_datetime(created_at_gt) if created_at_gt is not None else None,
                        "createdAt.lt": serialize_datetime(created_at_lt) if created_at_lt is not None else None,
                        "updatedAt.gt": serialize_datetime(updated_at_gt) if updated_at_gt is not None else None,
                        "updatedAt.lt": serialize_datetime(updated_at_lt) if updated_at_lt is not None else None,
                        "nextToken": next_token,
                        **(
                            request_options.get("additional_query_parameters", {})
                            if request_options is not None
                            else {}
                        ),
                    }
                )
            ),
            headers=jsonable_encoder(
                remove_none_from_dict(
                    {
                        **self._client_wrapper.get_headers(),
                        **(request_options.get("additional_headers", {}) if request_options is not None else {}),
                    }
                )
            ),
            timeout=request_options.get("timeout_in_seconds")
            if request_options is not None and request_options.get("timeout_in_seconds") is not None
            else self._client_wrapper.get_timeout(),
            retries=0,
            max_retries=request_options.get("max_retries") if request_options is not None else 0,  # type: ignore
        )
        try:
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        if 200 <= _response.status_code < 300:
            return pydantic_v1.parse_obj_as(PaginatedDatasets, _response_json)  # type: ignore
        if "error" in _response_json:
            if _response_json["error"] == "PaginationError":
                raise PaginationError(pydantic_v1.parse_obj_as(str, _response_json["content"]))  # type: ignore
        raise ApiError(status_code=_response.status_code, body=_response_json)

    async def download_dataset(
        self, dataset_id: DatasetId, *, request_options: typing.Optional[RequestOptions] = None
    ) -> str:
        """
        Parameters
        ----------
        dataset_id : DatasetId

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        str

        Examples
        --------
        from sphinxbio.client import AsyncSphinxbio

        client = AsyncSphinxbio(
            token="YOUR_TOKEN",
        )
        await client.datasets.download_dataset(
            dataset_id="ds_12345",
        )
        """
        _response = await self._client_wrapper.httpx_client.request(
            method="GET",
            url=urllib.parse.urljoin(
                f"{self._client_wrapper.get_base_url()}/", f"v1/datasets/{jsonable_encoder(dataset_id)}/download"
            ),
            params=jsonable_encoder(
                request_options.get("additional_query_parameters") if request_options is not None else None
            ),
            headers=jsonable_encoder(
                remove_none_from_dict(
                    {
                        **self._client_wrapper.get_headers(),
                        **(request_options.get("additional_headers", {}) if request_options is not None else {}),
                    }
                )
            ),
            timeout=request_options.get("timeout_in_seconds")
            if request_options is not None and request_options.get("timeout_in_seconds") is not None
            else self._client_wrapper.get_timeout(),
            retries=0,
            max_retries=request_options.get("max_retries") if request_options is not None else 0,  # type: ignore
        )
        try:
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        if 200 <= _response.status_code < 300:
            return pydantic_v1.parse_obj_as(str, _response_json)  # type: ignore
        if "error" in _response_json:
            if _response_json["error"] == "Redirect":
                raise Redirect()
            if _response_json["error"] == "ObjectNotFoundError":
                raise ObjectNotFoundError(
                    pydantic_v1.parse_obj_as(ObjectNotFoundBody, _response_json["content"])  # type: ignore
                )
        raise ApiError(status_code=_response.status_code, body=_response_json)

    async def create_dataset(
        self, *, request: CreateDataset, request_options: typing.Optional[RequestOptions] = None
    ) -> InitialTaskResponse:
        """
        Parameters
        ----------
        request : CreateDataset

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        InitialTaskResponse

        Examples
        --------
        from sphinxbio import CreateDataset
        from sphinxbio.client import AsyncSphinxbio

        client = AsyncSphinxbio(
            token="YOUR_TOKEN",
        )
        await client.datasets.create_dataset(
            request=CreateDataset(
                campaign_id="camp_67890",
                description="Dataset containing gene expression levels.",
                eln_link="https://eln.example.com",
                name="qPCR Results",
                staged_upload_path="unique-prefix/results.csv",
                tags=["exp001", "Gene Expression", "ML Ready"],
            ),
        )
        """
        _response = await self._client_wrapper.httpx_client.request(
            method="POST",
            url=urllib.parse.urljoin(f"{self._client_wrapper.get_base_url()}/", "v1/datasets"),
            params=jsonable_encoder(
                request_options.get("additional_query_parameters") if request_options is not None else None
            ),
            json=jsonable_encoder(request)
            if request_options is None or request_options.get("additional_body_parameters") is None
            else {
                **jsonable_encoder(request),
                **(jsonable_encoder(remove_none_from_dict(request_options.get("additional_body_parameters", {})))),
            },
            headers=jsonable_encoder(
                remove_none_from_dict(
                    {
                        **self._client_wrapper.get_headers(),
                        **(request_options.get("additional_headers", {}) if request_options is not None else {}),
                    }
                )
            ),
            timeout=request_options.get("timeout_in_seconds")
            if request_options is not None and request_options.get("timeout_in_seconds") is not None
            else self._client_wrapper.get_timeout(),
            retries=0,
            max_retries=request_options.get("max_retries") if request_options is not None else 0,  # type: ignore
        )
        try:
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        if 200 <= _response.status_code < 300:
            return pydantic_v1.parse_obj_as(InitialTaskResponse, _response_json)  # type: ignore
        if "error" in _response_json:
            if _response_json["error"] == "NonCSVFileError":
                raise NonCsvFileError(pydantic_v1.parse_obj_as(str, _response_json["content"]))  # type: ignore
            if _response_json["error"] == "InvalidStagedFileError":
                raise InvalidStagedFileError(
                    pydantic_v1.parse_obj_as(InvalidStagedFileBody, _response_json["content"])  # type: ignore
                )
        raise ApiError(status_code=_response.status_code, body=_response_json)
